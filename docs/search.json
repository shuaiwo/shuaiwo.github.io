[
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Shuai Wang",
    "section": "Education",
    "text": "Education\n\nUniversity of Pennsylvania, Philadelphia, PA (Aug 2023 â€“ Present)\nMaster of City Planning in Spatial Analytics\n\nRelevant courses: Geospatial Cloud Computing (SQL), Machine Learning in Remote Sensing (Python), Public Policy Analytics (R), Quantitative Analysis Methods (R)\n\n\n\nUniversity of Liverpool, Liverpool, UK\nBachelor of Arts in Urban Planning\n\nRelevant courses: Geospatial Data Science (Python), Urban & Environmental Economics"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Shuai Wang",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nTransport Planning and GIS Intern, Liverpool, UK (Jun - Sep 2022)\nMott Macdonald\n\n\nUrban Planning and Design Intern, Suzhou, China (Jul - Aug 2021)\nSuzhou Planning & Design Research Institute"
  },
  {
    "objectID": "resume.html#research-experience",
    "href": "resume.html#research-experience",
    "title": "Shuai Wang",
    "section": "Research Experience",
    "text": "Research Experience\n\nGeospatial Research Assistant, Philadelphia, PA (Dec 2023 - Present)\nBusiness and Conflict Barometer Initiative, The Wharton School\n\n\nUndergraduate Transport Researcher, Liverpool, UK (Apr 2022 - Jun 2023)\nSchool of Environmental Science, University of Liverpool\n\n\nUndergraduate Researcher, Suzhou, China (Jun - Sep 2021)\nDepartment of Urban Planning, Xiâ€™an Jiaotong-Liverpool University"
  },
  {
    "objectID": "resume.html#extracurricular-experience",
    "href": "resume.html#extracurricular-experience",
    "title": "Shuai Wang",
    "section": "Extracurricular Experience",
    "text": "Extracurricular Experience\n\nContent Planning Board Member, Philadelphia, PA (Sep 2023 - Present)\nWharton Future of Cities Conference"
  },
  {
    "objectID": "resume.html#skills-hobbies",
    "href": "resume.html#skills-hobbies",
    "title": "Shuai Wang",
    "section": "Skills & Hobbies",
    "text": "Skills & Hobbies\n\nTechnical skills: R (dplyr, tidyverse, etc.) | Python (pandas, numpy, etc.) | ArcGIS Pro, QGIS | Microsoft Excel | Adobe Illustrator & Photoshop\nInterests: Amateur baker, world traveling, exploring parks, hiking, photography in new cities"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "projects/Echoes_of_bias/index.html",
    "href": "projects/Echoes_of_bias/index.html",
    "title": "Echoes of Bias",
    "section": "",
    "text": "\"Hey ChatGPT, picture a doctor for me.\"\n Figure: Images of doctors created by ChatGPT using DALL-E (Author, 2024)\nI entered the same prompt 10 times, and the outcome was startlingly uniform - white and mostly men. On the contrary, 75.7% of the US healthcare practitioners including doctors are female, and 74.4% are white in 2022, according to the US Bureau of Labor Statistics. There is a significant representation of white masculinity in image generation models powered by Artificial Intelligence (AI).\nItâ€™s not just about doctors - this trend of white masculinity dominance spreads across AIâ€™s representation of various professions and categories. AI has reinforced the racial and gender disparities even more by tending to create images depicting individuals as white and male, particularly in scenarios involving authoritative roles, such as managers and CEOs.\nWhat does this tell us? Itâ€™s simple yet profound â€“ our AI mirrors our biases. The algorithms, fed with past data, often replicate societal stereotypes. AI aims to perform recurring data analysis and pattern-matching based on past reliable patterns and rule-based algorithms. Hereâ€™s the real kicker: as these AI models weave their way into city planning and governance, these biases donâ€™t just linger; they amplify. It is essential to apply AI in city planning and governance with caution to ensure it minimizes the risks of biases and fosters urban innovative and social inclusivity.\nApplying AI in city planning and governance presents bias and simplification of real-world issues. All data are inherently biased and have limitations as they are created by humans with either direct collection or indirect selection from human-placed things like sensors. As a consequence, the output result from the algorithm is likely to be biased as well, owing to the biased input data. For instance, the labels of pictures on ImageNet dataset contain biases of people as the input label data divide the images of humans into categories that can be discriminative, subjective and stereotypical. With the preset bias of people in the image labeling data, any algorithm that uses ImageNet as a data source can generate results that reinforce biased information. If biased personal data are widely used in city planning, governance and operations, it is very likely to pose a threat to the vulnerable groups that have already been suffering from biases. In addition, many social issues and challenges are too complex to be put into an algorithm. Plenty of urban problems cannot be simply captured by quantitative or qualitative data, as real-life problems consist of factors that cannot be measured precisely. There can only be a certain number of variables in one dataset, which also limits the ability of the algorithms to cover all the possible factors perfectly. Therefore, considering the potential consequences and challenges facing the application of AI in cities, it is necessary to use regulations to monitor the usage of AI.\nWhile the risks of using AI should be carefully monitored, AI has extensive opportunities and potential to enhance evidence-based decision-making in city planning and governance. By using the high quality and large quantity of existing training data, AI can contribute to informed and efficient decision-making. For instance, AI can be used to survey feedback from residents to measure their satisfaction with services or their feelings about local development impacts. This could help to identify areas where face-to-face, in-depth conversations are needed, which could further benefit from AI-supported pattern detection. Furthermore, AI can facilitate the optimization of urban systems and service operations. For example, AI can be applied to identify the ridership difference between female and male populations with large amounts of socio-economic and traffic data, which informs future policy-making and contributes to further optimization of urban services. Therefore, AI can benefit urban planning and operations with data-driven decision-making, such as improving efficiency and promoting innovation.\nAI practice in city planning, governance and operations should be regulated in a way that balances their benefits and harms, to minimize bias and to commit to innovation. For instance, the New York City AI Strategy intends to foster a thriving AI ecosystem and ensure the appropriate use of AI technology. The strategy aims to reduce potential biases by enhancing transparency, accountability, and public engagement in the development and deployment of AI solutions. Moreover, regulating the use of ML and AI does not mean stopping innovation in technology. In the New York City AI Strategy, the city supports ongoing AI literacy and skill-building needs by having robust educational infrastructure in place, and particular investment in place in public- and public-interest resources. In this way, by regulating the use of ML and AI cities, cities can foster a culture of innovation while protecting their citizens with privacy and inclusion.\nIn conclusion, AI offers great opportunities and potential to improve public engagement and evidence-based decision-making. Nevertheless, the essence of data bias and the simplification of real urban phenomena using AI can also threaten social equity and diversity. As a result, it is highly essential to use AI in cities with caution and care, such as minimizing bias and enhancing innovation. Furthermore, collaboration among various stakeholders in city development, such as planners, developers, policymakers, and researchers, should be encouraged to allow more transparency and up-to-date practice in AI. With the shared goal to implement AI more inclusively, cautiously and innovatively, the technology can contribute to the public good in the future."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis\n\n\n\n\n\n\nMobility\n\n\nData Analytics\n\n\nMachine Learning\n\n\nPrediction\n\n\n\n\n\n\n\nDec 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures\n\n\n\n\n\n\nData Analytics\n\n\nMachine Learning\n\n\nRemote Sensing\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Avenue Improvement Project Case Study\n\n\n\n\n\n\nTransportation\n\n\nResearch\n\n\nReport\n\n\n\n\n\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting NJ Transit Train Delays around NYC\n\n\n\n\n\n\nData Analytics\n\n\nMachine Learning\n\n\nPrediction\n\n\n\n\n\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial Risk Prediction in Philadelphia, PA\n\n\n\n\n\n\nData Analytics\n\n\nMachine Learning\n\n\nPrediction\n\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndego Bike Share Prediction in Philadelphia, PA\n\n\n\n\n\n\nData Analytics\n\n\nMachine Learning\n\n\nPrediction\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTarget Limited Outreach Resources for Home Repair Tax Credit Program in Philadelphia, PA\n\n\n\n\n\n\nData Analytics\n\n\nMachine Learning\n\n\nPrediction\n\n\n\n\n\n\n\nNov 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamining Transit-Oriented Development in Boston, MA\n\n\n\n\n\n\nData Analytics\n\n\nTransport\n\n\n\n\n\n\n\nSep 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWho are Low Traffic Neighborhoods Impacting?\n\n\n\n\n\n\nData Analytics\n\n\nGIS\n\n\nEssay\n\n\n\nData-driven Exploration of Social Equity of Low Traffic Neighborhoods in London, UK\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSocio-economic Analysis and Plan for Kirkby Town Centre, UK\n\n\n\n\n\n\nData Analytics\n\n\nGIS\n\n\nNeighborhood Plan\n\n\n\nMy contribution involves data analysis of democraphics, land use, retail and accessibility conditions; strategies of night-time economy and co-housing\n\n\n\nMay 30, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, Iâ€™m Shuai Wang ðŸ‘‹",
    "section": "",
    "text": "Iâ€™m a Masterâ€™s candidate studying City Planning and Spatial Data Analytics at the University of Pennsylvania. Iâ€™m passionate about leveraging the power of data science and technology for sustainable development and business growth. My research delves into urban technology, transportation and land use.\nTravelling has been a significant part of my life. Backpacking across 11 countries in Europe wasnâ€™t just an adventure; it was an immersive study into the fabric of different societies. Seeing firsthand how cultures, lifestyles, food and languages interact and blend across different places, Iâ€™m more determined to uncover the hidden dynamics that drive the development of cities, businesses and human behaviors."
  },
  {
    "objectID": "projects/Citibike project.html",
    "href": "projects/Citibike project.html",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "",
    "text": "Citi Bike is New York Cityâ€™s largest bike-sharing system, offering residents and tourists an affordable, sustainable, and efficient transportation alternative. Since its launch in 2013, Citi Bike has grown significantly, with thousands of bikes and hundreds of docking stations across Manhattan, Brooklyn, Queens, and other boroughs. As the demand for bike-sharing services continues to rise, there is a pressing need to analyze usage patterns, revenue generation, and network accessibility to enhance operational efficiency and user experience.\nThis project leverages trip data from Citi Bike for July 2024, focusing specifically on Manhattan. Manhattan serves as a critical area of analysis due to its high population density, diverse user base, and complex transportation dynamics. By analyzing trip data and integrating it with geospatial network information, this project aims to uncover key insights into ridership behavior, evaluate station accessibility, and identify opportunities to optimize the systemâ€™s operations and revenue."
  },
  {
    "objectID": "projects/Citibike project.html#ridership-and-membership-analysis",
    "href": "projects/Citibike project.html#ridership-and-membership-analysis",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "Ridership and Membership Analysis",
    "text": "Ridership and Membership Analysis\nThe Citi Bike data reveals intriguing temporal trends in daily ridership. The chart titled Daily Ridership Trends demonstrates that the highest ridership occurred on weekdays, particularly July 1st and July 10th, corresponding to Monday and Wednesday, respectively. These peaks reflect the systemâ€™s role in supporting Manhattanâ€™s weekday commuter base. The midweek ridership stability showcases Citi Bikeâ€™s seamless integration into the cityâ€™s transportation ecosystem for office workers, students, and other routine commuters.\nHowever, the sharp dip during the weekend (July 6thâ€“7th) signals an underutilized potential for leisure trips. With Manhattan hosting several tourist hotspots and vibrant weekend activities, the absence of a proportional rise in leisure ridership on weekends is noteworthy. This suggests that Citi Bike could improve its outreach to tourists and casual users who might otherwise opt for taxis or public transit. Offering targeted weekend packages, promoting bike tours, or collaborating with cultural and recreational events could help bridge this gap.\nInterestingly, the overall average of 90,000 daily trips highlights the resilience and reliability of Citi Bikeâ€™s operations. Despite varying weather conditions and external factors, the consistent demand underscores its importance to New Yorkers.\nThe membership distribution paints a striking picture of Citi Bikeâ€™s user base. Members contribute 77.3% of all trips, far outweighing the 22.7% share of casual users. This divide is both a strength and a vulnerability. While the loyalty and frequent usage of members stabilize revenue streams, the low proportion of casual riders points to untapped market potential.\nCritically, casual users are an important demographic for expanding Citi Bikeâ€™s reach. Their occasional use might stem from perceived barriers, such as unfamiliarity with the system or a lack of compelling benefits to join as members. By targeting these riders with discounts, loyalty programs, or easier onboarding processes, Citi Bike can create a pathway for casual users to transition into long-term subscribers. Creative solutions such as gamification (e.g., earning badges for rides) or bundling memberships with other services (e.g., public transit passes) could further bolster membership numbers.\n\n\nShow/Hide Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport geopandas as gpd\nfrom datetime import datetime\nimport requests\nimport osmnx as ox\nfrom shapely.geometry import Polygon, Point\nimport hvplot.pandas\nimport pandana as pdna\nimport  folium\nfrom branca.colormap import LinearColormap\nfrom folium.plugins import MousePosition\nimport xyzservices\n\npd.set_option('display.float_format', '{:.2f}'.format)\n# Load Citi Bike trip data (July 2024 dataset preview)\ndata_path1 = '202407-citibike-tripdata/202407-citibike-tripdata_1.csv'\ndata_path2 = '202407-citibike-tripdata/202407-citibike-tripdata_2.csv'\ndata1 = pd.read_csv(data_path1)\ndata2 = pd.read_csv(data_path2)\n\ntrips = pd.concat([data1, data2], ignore_index=True)\ntrips = trips[(trips['started_at'] &gt;= '2024-07-01') & (trips['started_at'] &lt;= '2024-07-15')]\n\n# Convert date columns to datetime format\ntrips['started_at'] = pd.to_datetime(trips['started_at'])\ntrips['ended_at'] = pd.to_datetime(trips['ended_at'])\n\n# Calculate trip duration in minutes\ntrips['trip_duration'] = (trips['ended_at'] - trips['started_at']).dt.total_seconds() / 60\n# Remove invalid trips (negative or zero duration)\ntrips = trips[trips['trip_duration'] &gt; 0]\n\n# Fetch Historical Weather Data using NOAA API\nnoaa_token = 'hiHwlLQdHGzOdZvDUHqynSFScHvhaaAd'  \nweather_url = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/data'\n\n# Function to fetch NOAA weather data\ndef fetch_noaa_weather(start_date, end_date, station_id):\n    headers = {'token': noaa_token}\n    params = {\n        'datasetid': 'GHCND',\n        'datatypeid': ['TMAX', 'TMIN', 'PRCP'],\n        'stationid': station_id,\n        'startdate': start_date,\n        'enddate': end_date,\n        'units': 'metric',\n        'limit': 1000\n    }\n    response = requests.get(weather_url, headers=headers, params=params)\n    if response.status_code == 200:\n        return pd.DataFrame(response.json()['results'])\n    else:\n        print(f\"Error: {response.status_code} {response.text}\")\n        return pd.DataFrame()\n\n# Fetch weather data for July 2024\nstation_id = 'GHCND:USW00094728'  # NOAA station ID for Central Park, NYC\nweather_data = fetch_noaa_weather('2024-07-01', '2024-07-31', station_id)\n\n# Pivot weather data to have TMAX, TMIN, and PRCP as columns\nweather_summary = weather_data.pivot_table(\n    index='date',\n    columns='datatype',\n    values='value',\n    aggfunc='mean'\n).reset_index()\n\nweather_summary.columns = ['date', 'precipitation', 'temp_max', 'temp_min']\nweather_summary['date'] = pd.to_datetime(weather_summary['date']).dt.date\n\n# Merge weather data with Citi Bike data\ntrips = trips.merge(weather_summary, left_on=trips['started_at'].dt.date, right_on='date', how='left')\n\n## Geospatial Data Integration\n# Load NYC bike network data\nG = ox.graph_from_place('Manhattan, New York, USA', network_type='bike')\n# Convert the network graph to GeoDataFrames\nnodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\nnyc = pd.read_csv('2020_Census_Tracts_20241221.csv')\nmanhattan = nyc[nyc['BoroCode'] == 1]\nfrom shapely.wkt import loads\nmanhattan['geometry'] = manhattan['the_geom'].apply(loads).drop(columns=['the_geom'])\nmanhattan = gpd.GeoDataFrame(manhattan, geometry='geometry', crs='EPSG:4326')\nmanhattan = manhattan[['CTLabel', 'geometry']]\n# Add bike lane coverage data\ntrips = gpd.GeoDataFrame(trips, geometry=gpd.points_from_xy(trips.start_lng, trips.start_lat), crs='EPSG:4326')\n# Perform spatial join to keep only trips in Manhattan\ntrips = gpd.sjoin(trips, manhattan, how='inner', op='intersects')\ntrips['nearest_node'] = ox.distance.nearest_nodes(G, trips.geometry.x, trips.geometry.y)\n# Get the list of valid nodes from the Manhattan graph G\nmanhattan_nodes = set(G.nodes)\n# Filter stations to include only those with a nearest node in the Manhattan graph\ntrips = trips[trips['nearest_node'].isin(manhattan_nodes)]\n# Reset the index after filtering\ntrips = trips.reset_index(drop=True)\n\n# Ridership and Membership Analysis\n# Daily ridership and member contributions\nrides_per_day = trips.groupby(trips['started_at'].dt.date)['ride_id'].count()\nmembers_vs_casuals = trips['member_casual'].value_counts()\n\n# Plot daily ridership trends\nplt.figure(figsize=(10, 6))\nrides_per_day.plot(kind='line', marker='o', color='blue')\n\n# Customize the plot\nplt.title('Daily Ridership Trends', fontsize=16)\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Number of Rides', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.xticks(rotation=45)\nplt.ylim(bottom=0)\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n# Plot membership distribution\nplt.figure(figsize=(8, 8))\nmembers_vs_casuals.plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightgreen'])\n\n# Customize the plot\nplt.title('Membership Distribution', fontsize=16)\nplt.ylabel('')  # Remove the default y-axis label\nplt.tight_layout()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "projects/Citibike project.html#weather-impact-on-customer-behavior",
    "href": "projects/Citibike project.html#weather-impact-on-customer-behavior",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "Weather Impact on Customer Behavior",
    "text": "Weather Impact on Customer Behavior\nThe influence of weather on ridership cannot be overstated. The chart Temperature vs.Â Ridership reveals a modest decline in ridership as temperatures exceed 30Â°C, emphasizing the discomfort associated with cycling during extreme heat. Riders appear to prefer moderate temperatures, with the sweet spot lying between 27Â°C and 29Â°C. This highlights a unique challenge: while bike usage peaks during favorable weather conditions, extreme heatâ€”common during New Yorkâ€™s summersâ€”may reduce ridership.\nThe chart Precipitation vs.Â Ridership offers more insights. Days with any significant rainfall experienced moderate reductions in ridership, while some slight rain has very limited impact on ridership. This underscores the reliance of Citi Bike on fair-weather riders and raises the question of how to maintain ridership during adverse conditions.\nCiti Bike could mitigate these impacts by implementing weather-responsive strategies. For example, offering discounts or loyalty points on rainy days could encourage usage even during light rain. Additionally, providing weather protection gear such as rain ponchos or collaborating with local stores to offer discounts for riders could reduce the deterrent effect of precipitation.\n\n\nShow/Hide Code\n# Analyze ridership under different weather conditions\nweather_impact = trips.groupby('date').agg(\n    ride_count=('ride_id', 'size'),\n    max_temp=('temp_max', 'max'),\n    total_precip=('precipitation', 'sum')\n).reset_index()\n\n# Line plot with regression: Temperature vs. Average Daily Ridership\nsns.lmplot(x='max_temp', y='ride_count', data=weather_impact, aspect=2, height=6, scatter_kws={'color':'blue'}, line_kws={'color':'red'})\nplt.title('Temperature vs. Ridership with Trend Line (7/1 to 7/15)')\nplt.xlabel('Max Temperature (Â°C)')\nplt.ylabel('Ride Count')\nplt.ylim(bottom=0)\nplt.show()\n# Scatter plot: Precipitation vs. Ridership\nplt.figure(figsize=(12, 6))\nsns.scatterplot(x='total_precip', y='ride_count', data=weather_impact, color='green')\nplt.title('Precipitation vs. Ridership (7/1 to 7/15)')\nplt.xlabel('Total Precipitation (mm)')\nplt.ylabel('Ride Count')\nplt.ylim(bottom=0)\nplt.show()"
  },
  {
    "objectID": "projects/Citibike project.html#revenue-analysis",
    "href": "projects/Citibike project.html#revenue-analysis",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "Revenue Analysis",
    "text": "Revenue Analysis\nThe horizontal bar chart Top 10 Stations by Revenue underscores a stark revenue concentration at a few key stations. Stations near Central Park (7 Ave & Central Park South, Central Park S & 6 Ave) consistently generated the highest revenue, followed closely by West St & Chambers St in the financial district. These findings highlight the dual role of Citi Bike stations: supporting commuting in business hubs and catering to leisure activities near tourist destinations.\nInterestingly, the revenue disparity between high-performing and low-performing stations raises questions about station placement. While some stations thrive due to their strategic locations, others may struggle due to poor connectivity, low foot traffic, or limited bike availability. By analyzing underperforming stations in conjunction with ridership patterns, Citi Bike could optimize station placement and redistribute underused assets to high-demand areas.\nFurthermore, the concentration of revenue at a few stations underscores the need for dynamic resource allocation. High-performing stations require frequent bike redistribution and maintenance to ensure uninterrupted service during peak hours.\n\n\nShow/Hide Code\n# Pricing logic based on Citi Bike pricing\n# Single Ride: $4.79 for 30 minutes, $0.36/min thereafter\n# Day Pass: $19/day with unlimited 30-minute rides, $4 every 15 minutes after 30 minutes\n# Citi Bike Annual Member: $219.99/year, $0.24/min for e-bikes\n# Lyft Pink Member: $199/year, $0.24/min for e-bikes\ndef calculate_revenue(row):\n    if row['member_casual'] == 'casual':\n        if row['trip_duration'] &lt;= 30:\n            return 4.79  # Flat fee for single rides\n        else:\n            return 4.79 + 0.36 * (row['trip_duration'] - 30)  # Extra charge after 30 mins\n    elif row['member_casual'] == 'member':\n        return 0.24 * row['trip_duration']  # Per-minute charge for e-bikes\n    else:\n        return 0  # Default if no valid category\n    \ntrips['revenue'] = trips.apply(calculate_revenue, axis=1)\ntotal_revenue = trips['revenue'].sum()\n#print(f'Total Revenue: ${total_revenue}')\n# Station Performance\nstation_revenue = trips.groupby('start_station_name')['revenue'].sum().sort_values(ascending=False)\ntop_stations = station_revenue.head(10)\n# Plot a horizontal bar chart\nplt.figure(figsize=(10, 6))\ntop_stations.plot(kind='barh', color='skyblue', edgecolor='black')\n# Add labels and title\nplt.title('Top 10 Stations by Revenue', fontsize=16)\nplt.xlabel('Revenue ($)', fontsize=12)\nplt.ylabel('Station Name', fontsize=12)\nplt.gca().invert_yaxis()  # Invert y-axis for readability\nplt.grid(axis='x', linestyle='--', alpha=0.7)\n# Show the plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/Citibike project.html#customer-segmentation",
    "href": "projects/Citibike project.html#customer-segmentation",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "Customer Segmentation",
    "text": "Customer Segmentation\nThe table Customer Segments provides an in-depth look at Citi Bikeâ€™s user base, categorized into two distinct clusters. Casual users, represented by Cluster 0, take significantly longer trips (24.12 minutes on average) but contribute fewer total trips (277,748). This cluster aligns with leisure-oriented riders who likely use Citi Bike for recreation or occasional errands.\nIn contrast, Cluster 1, dominated by members, reflects a commuter-driven behavior. With an average trip duration of 12.23 minutes and a total trip count of 945,056, this cluster underscores the reliance of Citi Bike on short, frequent trips by daily commuters. Members likely use Citi Bike as a last-mile solution, connecting subway stations or buses to their workplaces.\nThese insights have profound implications for marketing and operational strategies. For casual users, Citi Bike could promote longer leisure rides through partnerships with tourist attractions or cultural sites. For members, enhancing commuter-centric services, such as guaranteed bike availability during rush hours, could further cement their loyalty.\n\n\nShow/Hide Code\n# K-Means Clustering for Customer Segmentation\n# Features: Average trip duration, trip count\nuser_features = trips.groupby('member_casual').agg(\n    avg_duration=('trip_duration', 'mean'),\n    trip_count=('ride_id', 'count')\n).reset_index()\n\nkmeans = KMeans(n_clusters=2, random_state=42)\nuser_features['cluster'] = kmeans.fit_predict(user_features[['avg_duration', 'trip_count']])\n\nfrom IPython.display import display\n\n# Define a mapping for cluster labels to make them more descriptive\ncluster_labels = {\n    0: \"High Duration, Low Trip Count (Casual Users)\",\n    1: \"Low Duration, High Trip Count (Members)\"\n}\n\n# Add descriptive cluster labels\nuser_features['cluster_label'] = user_features['cluster'].map(cluster_labels)\n\n# Format the table with renamed columns for better presentation\nsegmentation_table = user_features.rename(columns={\n    'member_casual': 'User Type',\n    'avg_duration': 'Average Trip Duration (mins)',\n    'trip_count': 'Total Trip Count',\n    'cluster_label': 'Segment Description'\n}).drop(columns=['cluster'])\n\n# Display the table\nprint(\"\\nCustomer Segmentation Table:\")\ndisplay(segmentation_table)\n\n\n\nCustomer Segmentation Table:\n\n\n\n\n\n\n\n\n\nUser Type\nAverage Trip Duration (mins)\nTotal Trip Count\nSegment Description\n\n\n\n\n0\ncasual\n24.12\n277748\nHigh Duration, Low Trip Count (Casual Users)\n\n\n1\nmember\n12.23\n945056\nLow Duration, High Trip Count (Members)"
  },
  {
    "objectID": "projects/Citibike project.html#station-performance-and-connectivity-analysis",
    "href": "projects/Citibike project.html#station-performance-and-connectivity-analysis",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "Station Performance and Connectivity Analysis",
    "text": "Station Performance and Connectivity Analysis\nThe distribution of total trips reveals a strong concentration of high-performing stations in central Manhattan, particularly in areas such as Midtown, Central Park, and the Financial District. Stations near Central Park, including 7 Ave & Central Park South and Central Park S & 6 Ave, continue to stand out as major hubs, aligning with previous revenue analyses. These stations attract a mix of commuter and leisure riders, leveraging their strategic location near residential areas, office spaces, and tourist attractions.\nOn the other hand, stations in the Upper Manhattan region and the peripheral areas of Lower Manhattan demonstrate relatively lower activity. This discrepancy may be attributed to a combination of factors, such as lower population density, limited tourist attractions, or weaker connectivity to bike lanes and public transit hubs. For example, stations along the northern parts of the island might benefit from targeted campaigns to encourage usage among nearby residents and commuters.\nThe analysis also highlights several outlier stations with unusually high total trips. These stations may serve as critical redistribution points in the network, playing an outsized role in balancing bike availability across Manhattan. Such patterns underscore the need for Citi Bike to focus resources on maintaining and scaling these high-demand locations.\nA significant and promising finding is that all Citi Bike stations in Manhattan are connected to a bike lane within 100 meters. This high level of connectivity is critical for ensuring the systemâ€™s usability and safety. The seamless integration with Manhattanâ€™s extensive bike lane network provides riders with a safe and convenient experience, minimizing interactions with vehicle traffic.\nThe proximity to bike lanes plays a pivotal role in encouraging ridership, particularly for casual users who may feel less confident cycling in mixed traffic conditions. Stations with strong connectivity to bike lanes often exhibit higher trip counts, reinforcing the importance of station placement near well-developed cycling infrastructure.\nFrom a policy perspective, these findings validate the cityâ€™s ongoing efforts to expand and enhance its bike lane network. Further investments in bike lane infrastructure, particularly in regions with low ridership, could help improve station utilization and support Citi Bikeâ€™s growth.\n\n\nShow/Hide Code\n# Calculate average ridership per station\n#station = trips.groupby('start_station_id')['ride_id'].count().reset_index()\nstation = trips.groupby(['start_station_name', 'start_station_id']).agg({\n    'ride_id': 'count',                # Total trips\n    'geometry': 'first',  # Keep the geometry of each station\n    'trip_duration': 'mean',           # Average trip duration\n    'precipitation': 'mean',           # Average precipitation\n    'temp_max': 'mean',                # Average max temperature\n    'geometry': 'first',               # Keep the first geometry for mapping\n    'nearest_node' : 'first',\n}).reset_index()\n\n# Rename columns for clarity\nstation.rename(columns={\n    'ride_id': 'total_trips',\n    'trip_duration': 'avg_trip_duration',\n    'precipitation': 'avg_precipitation',\n    'temp_max': 'avg_temp_max'\n}, inplace=True)\n# Ensure the result is a GeoDataFrame\nstation = gpd.GeoDataFrame(station, geometry='geometry',crs='EPSG:4326')\n# Reproject stations and edges to a CRS with units\nedges = edges.to_crs(epsg=4326)\n# Calculate the nearest edge for each station\nstation['nearest_edge'] = station.geometry.apply(\n    lambda geom: ox.distance.nearest_edges(G, geom.x, geom.y)\n)\n# Ensure 'u' and 'v' columns exist by resetting the index\nedges = edges.reset_index()\n# Extract the geometry of the nearest edge for each station\nstation['nearest_edge_geom'] = station['nearest_edge'].apply(\n    lambda edge: edges.loc[(edges['u'] == edge[0]) & (edges['v'] == edge[1]), 'geometry'].values[0]\n)\n# Get the geometry of the nearest edge for each station\nstation['nearest_edge_geom'] = station['nearest_edge'].apply(\n    lambda edge: edges.loc[(edges['u'] == edge[0]) & (edges['v'] == edge[1]), 'geometry'].values[0]\n)\n# Calculate the distance to the nearest bike lane\nstation['distance_to_bike_lane'] = station.apply(\n    lambda row: row.geometry.distance(row.nearest_edge_geom), axis=1\n)\n\n# Add connectivity flag: 1 if connected (within 50 meters), 0 otherwise\nstation['connected_to_bike_lane'] = (station['distance_to_bike_lane'] &lt;= 500).astype(int)\n# Summary statistics\nconnected_count = station['connected_to_bike_lane'].sum()\ntotal_stations = len(station)\n#print(f\"Connected stations: {connected_count} / {total_stations} ({(connected_count / total_stations) * 100:.2f}%)\")\n# Drop the `nearest_edge_geom` column to avoid serialization issues\nstation_m = station.drop(columns=['nearest_edge_geom'])\nm = station_m.explore(\n    column='total_trips',  # Color stations by total trips\n   # cmap='Accent',\n    tooltip=['start_station_name', 'total_trips', 'avg_trip_duration'],\n    style_kwds={\"weight\": 2,  \"fillOpacity\": 0.6},\n    name=\"Stations Coverage\",\n    tiles=xyzservices.providers.CartoDB.DarkMatter,\n)\n# Add a base map using folium\nmap_center = [station_m.geometry.y.mean(), station_m.geometry.x.mean()]\n#base_map = folium.Map(location=map_center, zoom_start=12, tiles=\"CartoDB.Positron\")\n# Add the street network to the map\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "projects/Citibike project.html#conclusion-and-recommendations",
    "href": "projects/Citibike project.html#conclusion-and-recommendations",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "Conclusion and Recommendations",
    "text": "Conclusion and Recommendations\nThis spatial analysis underscores the success of Citi Bikeâ€™s station placement strategy in Manhattan, particularly its integration with the cityâ€™s bike lane network. The combination of strong connectivity, strategic locations, and dual-purpose usage patterns for commuters and leisure riders makes Citi Bike an indispensable part of New York Cityâ€™s transportation landscape. By addressing disparities in station performance and continuing to align with Manhattanâ€™s bike lane expansion, Citi Bike can further enhance its usability, sustainability, and user satisfaction. Here are the recommendations from the analysis to improve the operations of Citibikes:\n\nExpand Infrastructure in High-Demand Areas Central Park and Midtown:The analysis highlights that stations near Central Park and Midtown are consistently high-performing in terms of revenue and trip counts. Expanding capacity at these stations by adding more docks and bikes can alleviate pressure during peak times. Introducing additional stations in nearby areas can distribute demand more evenly and reduce congestion. Financial District: Stations in the financial district also generate significant revenue, likely driven by commuting patterns. Adding more stations or expanding existing ones in this area can cater to the growing commuter demand.\nOptimize Underperforming Stations Northern and Peripheral Manhattan: Launch targeted marketing campaigns to raise awareness about these stations; Partner with local businesses or community organizations to integrate these stations into daily life. Relocation or Redistribution: For persistently underperforming stations, consider relocating them to areas with higher foot traffic or better connectivity to bike lanes.\nLeverage Weather-Responsive Strategies Weather-Based Incentives: Ridership drops significantly during rainy days and extreme heat. Offering discounts or loyalty points on rainy days could encourage usage, even in unfavorable weather conditions. Gear and Comfort Enhancements: Provide riders with rain ponchos or collaborate with local stores to offer discounts on weather gear for riders.; Introduce shaded docking stations in high-traffic areas to improve comfort during summer.\nStrengthen Member Engagement Convert Casual Users to Members: Casual users contribute 22.7% of trips but have longer trip durations. This group represents an opportunity to grow membership by: Offering discounted membership trials; Highlighting member-exclusive benefits, such as extended trip durations or priority access during peak times."
  },
  {
    "objectID": "projects/Citibike project.html#background",
    "href": "projects/Citibike project.html#background",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "",
    "text": "Citi Bike is New York Cityâ€™s largest bike-sharing system, offering residents and tourists an affordable, sustainable, and efficient transportation alternative. Since its launch in 2013, Citi Bike has grown significantly, with thousands of bikes and hundreds of docking stations across Manhattan, Brooklyn, Queens, and other boroughs. As the demand for bike-sharing services continues to rise, there is a pressing need to analyze usage patterns, revenue generation, and network accessibility to enhance operational efficiency and user experience.\nThis project leverages trip data from Citi Bike for July 2024, focusing specifically on Manhattan. Manhattan serves as a critical area of analysis due to its high population density, diverse user base, and complex transportation dynamics. By analyzing trip data and integrating it with geospatial network information, this project aims to uncover key insights into ridership behavior, evaluate station accessibility, and identify opportunities to optimize the systemâ€™s operations and revenue."
  },
  {
    "objectID": "projects/Citibike project.html#motivation",
    "href": "projects/Citibike project.html#motivation",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "Motivation",
    "text": "Motivation\nThe motivation behind this project stems from several key factors:\n\nSustainability and Urban Mobility: Citi Bike plays a vital role in promoting sustainable transportation in one of the worldâ€™s busiest cities. Understanding usage patterns and station accessibility can help policymakers and planners expand the bike network effectively, reducing car dependency and lowering carbon emissions.\nData-Driven Optimization: Citi Bikeâ€™s operations involve complex logistical challenges, including balancing bike distribution across stations, maintaining infrastructure, and optimizing pricing models. This project uses data-driven methods to evaluate station coverage, ridership trends, and revenue streams, providing actionable recommendations for operational improvement.\nImproving User Experience: By analyzing trip durations, weather impacts, and station-level trends, the project aims to identify factors influencing user satisfaction. Insights into peak usage times, station demand, and accessibility gaps can help enhance user convenience and attract more riders."
  },
  {
    "objectID": "projects/Citibike project.html#goals-and-objectives",
    "href": "projects/Citibike project.html#goals-and-objectives",
    "title": "Optimizing Bike-Sharing Operations: Insights of CitiBike in Manhattan Using Data-Driven Analysis",
    "section": "Goals and Objectives",
    "text": "Goals and Objectives\n\nRidership Analysis: Analyze daily and hourly trends in Citi Bike trips to identify peak times and seasonal usage patterns.\nStation Accessibility: Evaluate station coverage by integrating trip data with Manhattanâ€™s bike network, calculating metrics like nearest nodes and coverage buffers.\nRevenue Assessment: Calculate revenue based on Citi Bikeâ€™s pricing structure, identifying top-performing stations and ride types that contribute to financial success.\nImpact of Weather: Investigate the relationship between weather conditions (e.g., temperature and precipitation) and ridership behavior."
  },
  {
    "objectID": "projects/PA-CRD-2024.html",
    "href": "projects/PA-CRD-2024.html",
    "title": "Spatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures - An Approach Using Earth-Engine-Based Remote Sensing Data and Air Quality Variables in the State of Pennsylvania",
    "section": "",
    "text": "Emily Zhou, Shuai Wang\nDepartment of City and Regional Planning, Weitzman School of Design, University of Pennsylvania\nVersion 3.0 | First Created Apr 1, 2024 | Updated May 03, 2024\nKeywords: support vector machine, random forest, multiple layer perceptron, deep learning, bayesian information criteria, google earth engine, geospatial health\nImportant Links: original paper, project repository"
  },
  {
    "objectID": "projects/PA-CRD-2024.html#introduction",
    "href": "projects/PA-CRD-2024.html#introduction",
    "title": "Spatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures - An Approach Using Earth-Engine-Based Remote Sensing Data and Air Quality Variables in the State of Pennsylvania",
    "section": "1. Introduction",
    "text": "1. Introduction\nIn the past few years, remote sensing data have increasingly been used in monitoring, spatial predicting modeling, and risk assessment with respect to human health for their ability to facilitate the identification of social and environment determinants of health and to provide consistent spatial and temporal coverage of study data. In the meantime, machine learning techniques have been widely used in public health research to enhance the accuracy of disease prediction and early detection, improve the efficiency of healthcare resource allocation, and enable the development of personalized treatment plans.\nAs an example of applying remote sensing data and machine learning techniques in health research, Alvarez-Mendoza et al.Â conducted a study in 2020, in which they proposed to estimate the prevalence of Chronic Respiratory Diseases (CRDs) by examining the relationship between remote sensing data, air quality variables, and the number of hospital discharges of patients with CRDs in Quito, Ecuador. Specifically, their study estimated and compared three different complex machine learning techniques, support vector regression, random forest regression, and multiple layer perceptron, in predicting CRD rate, considering the effect of environmental variables, air pollution field measurements, and meteorological data. Their goal is to provide insights into and an understanding of the most significant spatial predictors and the spatial distribution of HCRD in the city of Quito.\nFor a detailed description of Alvarez-Mendoza et alâ€™s methodology and workflow, please refer to the workflow below and the presentation slides here.\n\n\n\nworkflow\n\n\nIn our final project, we plan to replicate and improve upon Alvarez-Mendoza et al.â€™s study and investigate the effectiveness of several machine learning models in predicting the number of hospital discharge patients with CRD in the state of Pennsylvania. Following their established workflow, we combined multiple data sources, including specific bands of satellite imageries, different kinds of vegetation indices, monthly air pollutant measures, and meterological indicators, as proxies for local environment to analyze the distribution of CRD hospitalizations across the Pennsylvania. Our goal is to understand the most significant environmental and atmospheric factors leading to higher CRD risk in Pennsylvania as well as to compare the performance of different machine learning models.\nOur biggest deviation from Alvarez-Mendoza et al.â€™s study is to conduct the analysis at a much larger geographic extent. Since our raw data all comes from different sources and with different geographic unit (eg. hospital discharge data are collected a county level while remote sensing data are at 30*30m spatial resolution), we divide Pennsylvania into 8397 5000*5000m contiguous fishnet grids. Doing so provide a regular and systematic way to divide geographic areas into smaller units. It simplifies our analysis by providing a structured framework for organizing and processing spatial data. Another major deviation from the original study is the availability of air quality data in Pennsylvania. Because the influence of air quality monitoring stations is not limited by administrative boudaries, making spatial interpolation of their influence zone becomes an important step before analysis. We computed several voronoi polygons based on station locations that collect different kinds of air quality data. Each station is associated with a polygon representing the area where it has the closet proximity, from which we could then aggregate to the fishnet. The third deviation is that we took seasonality into account while running the machine learning model. Specifically, models were ran on data from different seasons separately for cross-comparison. For reference, here is our modified workflow, with the modified part highlited in green.\n\n\n\nworkflow2\n\n\nThis notebook documents our entire replication study and is organized into the following sections following the workflow of Alvarez-Mendoza et al.: - Study Area: where we document the procedure of generating the fishnet and defining our area of interest.\n- Remote Sensing Data: where we document our process of retreving, fine-tuning, and manipulating satellite images into format ready for further analysis. We also explains the method to calculate several indices that are used in our model. - Hospital Discharge Data: where we document our process of cleaning hospital discharge CRD data and aggregating it to the fishnet. - Air Quality Data: where we document our process of building voronoi polygon and around stations and aggregating station information into the fishnet. - Dimensionality Reduction: where we ran the bayesian information criteria to select the most influential predictors of CRD in order to prevent multicolinearity and overfitting. - Machine Learning: where we ran four machine learning models for each season and compare their accuracy.\n\n\nCode\n## Import Function Libraries\nimport ee\nimport geemap\nimport requests\nfrom datetime import datetime\nimport geopandas as gpd\nfrom shapely import geometry\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom shapely.geometry import shape, Polygon\nimport json\nimport seaborn as sns\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport tensorflow as tf\nfrom scipy.stats import norm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nCode\n## Set Up Credentials\nee.Authenticate()\nee.Initialize(project=\"gee-emilyzhou0112\") # replace the project here with your own ee project name\n\n\n\n            \n            \n\n\nNote: all data used in this replication study are stored in our GitHub repository. However, since the first version of this notebook is hosted on google drive, some of the earth engine functions will write result into google drive by default. If you would like to reproduce our results, please do not run these code as specified by the insturctions and instead, load the processed data from our repository."
  },
  {
    "objectID": "projects/PA-CRD-2024.html#study-area",
    "href": "projects/PA-CRD-2024.html#study-area",
    "title": "Spatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures - An Approach Using Earth-Engine-Based Remote Sensing Data and Air Quality Variables in the State of Pennsylvania",
    "section": "2. Study Area",
    "text": "2. Study Area\nTo limit our study area to the state of Pennsylvania, we retrieved the geojson file of all Pennsylvania counties from the Open Data Pennsylvania. Next, the code dissolves the geometries of all features in the GeoDataFrame pa into a single geometry representing the boundary of Pennsylvania. Then, it extracts the exterior coordinates of the dissolved boundary geometry and converts them into a list format suitable for google earth engine. This list of coordinates is used to create a polygon geometry pa_geom using the ee.Geometry.Polygon() function from the Earth Engine Python API.\nThe polygon geometry representing the boundary of Pennsylvania is converted into an Earth Engine FeatureCollection (aoi). This FeatureCollection serves as the study area extent for subsequent analysis within Google Earth Engine and is used later in the study to clip the satellite images.\n\n\nCode\n## Load Study Area\ngeojson_path= 'https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/raw/pa.geojson'\npa = gpd.read_file(geojson_path)\npa_bound = pa.dissolve() # dissolve geometry to get the boundary\npa_geom= ee.Geometry.Polygon(list(pa_bound['geometry'].iloc[0].exterior.coords)) # convert the geometry into a format suitable for gee\naoi = ee.FeatureCollection(pa_geom) # create earth engine feature collection for study area\n\n\n\n            \n            \n\n\nBefore creating the fishnet, we need to reproject our state boundary into EPSG:3857, the projected coordinate system, from which we could define the minimum and maximum bounds.\nUsing a nested loop, the code iterates over the X and Y coordinates within the bounds of the reprojected boundary. Within each iteration, it creates a square polygon geometry representing a grid cell. These polygons are constructed using the Shapely libraryâ€™s Polygon function, with each square having sides of 5000 meter. This is an arbitrary number that we came up, considering the spatial resolution of satellite images, total number of grids, and the run time of spatial aggregation later in the analysis. Once all grid cells are generated, a GeoDataFrame fishnet is created from the list of geometries, with the coordinate reference system set to EPSG:3857.\n\n\nCode\n## Generate Fishnet\npa_proj = pa_bound.to_crs('EPSG:3857') # reproject to projected coordinate system\nminX, minY, maxX, maxY = pa_proj.total_bounds\n\ngeom_array = []\nsquare_size = 5000\ny = minY\nwhile y &lt;= maxY:\n    x = minX\n    while x &lt;= maxX:\n        geom = geometry.Polygon([(x, y), (x, y + square_size), (x + square_size, y + square_size), (x + square_size, y), (x, y)])\n        geom_array.append(geom)\n        x += square_size\n    y += square_size\nfishnet = gpd.GeoDataFrame(geometry=geom_array, crs='EPSG:3857')\n\n\nSpatial vector operations are performed on two layers with the same projection. That said, we need to project fishnet back to EPSG:4326 to match with other spatial data we have. We clip the reprojected fishnet grid to the extent of Pennsylvania pa_bound. This clipping operation ensures that the fishnet grid is constrained within the boundaries of Pennsylvania, effectively limiting the grid to cover only the geographic area of interest.\n\n\nCode\n## Clip Fishnet\nfishnet = fishnet.to_crs(epsg=4326)\npa_fishnet = gpd.clip(fishnet, pa_bound) # to the extent of PA\n\n\nThe visualization below shows the fishnet we created. Upon examining the grids, we notice that there are some overlapping lineworks in the northen part of the state. This will likely lead to some inaccuracies or even errors during data aggregation, but we will dive into this later in the report.\n\n\nCode\n# Visualize Fishnet\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_title('Fishnet Grid over Pennsylvania')\npa_fishnet.plot(ax=ax, cmap='viridis', edgecolor='white')\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\nLoad Processed Fishnet Data Below (Optional)\nAlternatively, you may also load in the pa_fishnet file directly from our GitHub repository to save time.\n\n\nCode\npa_fishnet = gpd.read_file('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/pa_fishnet.geojson')"
  },
  {
    "objectID": "projects/PA-CRD-2024.html#remote-sensing-data",
    "href": "projects/PA-CRD-2024.html#remote-sensing-data",
    "title": "Spatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures - An Approach Using Earth-Engine-Based Remote Sensing Data and Air Quality Variables in the State of Pennsylvania",
    "section": "3. Remote Sensing Data",
    "text": "3. Remote Sensing Data\nWe acquired all Landsat 8 level 2 images satellite images from Spring 2022 to Spring 2023 over the study area. These images were obtained via the Google Earth Engine API and are coded in the Earth Engine Data Catalog as USGS Landsat 8 Level 2, Collection 2, Tier 1. We acquired those images by season and took the average before further processing.\nThis specific version and level of processing of Landsat 8 satellite imagery provided by the United States Geological Survey uses Surface Reflectance Code to generate products with geometrical, radiometric, and atmospheric corrections. These products have a spatial resolution of 30 m. The products used in this study as predictors are the surface reflectance OLI bands, brightness temperature (BT), and some pre-processed indexes, such as the normalized difference vegetation index (NDVI), the soil-adjusted vegetation index (SAVI), and the enhanced vegetation index (EVI). Moreover, the images were processed to scale alll the bands and remove cloud coverage.\nLetâ€™s first set up the constants that define the temporal extent of our study and transform them into a format that Earth Engine expects.\n\n\nCode\n## Define Time Period\nstartSpring = datetime(2022, 3, 1) # spring\nendSpring = datetime(2022, 5, 31)\nstartSummer = datetime(2022, 6, 1) # summer\nendSummer = datetime(2022, 8, 31)\nstartFall = datetime(2022, 9, 1) # fall\nendFall = datetime(2022, 11, 30)\nstartWinter = datetime(2022, 12, 1) # winter\nendWinter = datetime(2023, 2, 28)\n\n# Format dates into strings that Earth Engine expects (\"YYYY-MM-DD\")\nstartSpring= startSpring.strftime('%Y-%m-%d')\nendSpring = endSpring.strftime('%Y-%m-%d')\nstartSummer = startSummer.strftime('%Y-%m-%d')\nendSummer = endSummer.strftime('%Y-%m-%d')\nstartFall = startFall.strftime('%Y-%m-%d')\nendFall = endFall.strftime('%Y-%m-%d')\nstartWinter = startWinter.strftime('%Y-%m-%d')\nendWinter = endWinter.strftime('%Y-%m-%d')\n\n\nThe function apply_scale_factors applies scale and offset adjustments to Landsat satellite imagery bands, specifically for optical and thermal bands. In the Landsat data, pixel values are often represented as digital numbers (DN) which require conversion to physical units like reflectance or temperature.\n\n\nCode\n## Helper Function - Scale Bands\ndef apply_scale_factors(image):\n    # Scale and offset values for optical bands\n    optical_bands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n\n    # Scale and offset values for thermal bands\n    thermal_bands = image.select('ST_B.*').multiply(0.00341802).add(149.0)\n\n    # Add scaled bands to the original image\n    return image.addBands(optical_bands, None, True) \\\n                .addBands(thermal_bands, None, True)\n\n\nThe cloud_mask function is designed to create a binary mask for identifying and masking out pixels affected by clouds and cloud shadows in Landsat satellite imagery. It plays a crucial role in pre-processing Landsat imagery by removing cloud and cloud shadow effects to enhance data quality and reliability for downstream analysis.\n\n\nCode\n## Helper Function - Mask Clouds\ndef cloud_mask(image):\n    # Define cloud shadow and cloud bitmask (Bits 3 and 5)\n    cloud_shadow_bit_mask = 1 &lt;&lt; 3\n    cloud_bit_mask = 1 &lt;&lt; 5\n\n    # Select the Quality Assessment (QA) band for pixel quality information\n    qa = image.select('QA_PIXEL')\n\n    # Create a binary mask to identify clear conditions (both cloud and cloud shadow bits set to 0)\n    mask = qa.bitwiseAnd(cloud_shadow_bit_mask).eq(0) \\\n                .And(qa.bitwiseAnd(cloud_bit_mask).eq(0))\n\n    # Update the original image, masking out cloud and cloud shadow-affected pixels\n    return image.updateMask(mask)\n\n\n\n3.1 Prepare Spring Data\nThis section below documents our process of loading a springtime image collection from the Landsat 8 Level 2, Collection 2, Tier 1 dataset using the Google Earth Engine (GEE) Python API, aggregating the data into the fishnet, and joining them all together into one dataframe.\nThe code snippet loads spring season image series, applies scale factors and cloud masking, calculates a median composite image, and clips it to the specified area of interest, preparing the data for further analysis.\n\n\nCode\n## Load Spring Image Collection\nimageSpring = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n            .filterBounds(aoi) \\\n            .filterDate(startSpring, endSpring) \\\n            .map(apply_scale_factors) \\\n            .map(cloud_mask) \\\n            .median() \\\n            .clip(aoi)\n\n\nThe geemap package allows us to visualize the median of all satellite images library from March 2022 to may 2022.\n\n\nCode\n## Visualize Spring Image Collection\nMap = geemap.Map()\nMap.addLayer(aoi, {}, 'AOI')\nMap.centerObject(aoi, 10)\n\nvisualization = {\n  'bands': ['SR_B4', 'SR_B3', 'SR_B2'],\n  'min': 0.0,\n  'max': 0.15,\n}\nMap.addLayer(imageSpring, visualization, 'True Color 432')\nMap\n\n\nAs we breifly mentioned in the introduction, we acquired four different indices based on the bands of our satellite images. They are NDVI, SAVI, EVI, and LST. This code segment calculates the Normalized Difference Vegetation Index (NDVI) for the spring image collection. NDVI is a common vegetation index used to assess the presence and health of vegetation based on the difference in reflectance between near-infrared (NIR) and red light wavelengths.\nThe formula is Â NDVI = (Band 5 â€“ Band 4) / (Band 5 + Band 4)\n\n\nCode\n## Calculate Normalized Difference Vegetation Index (NDVI)\nndvi_spring = imageSpring.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\n\n\nWe may view the metadata information for the ndvi_spring image, including properties such as band names, data type, projection, scale, and other relevant information about the image computed from the NDVI calculation.\n\n\nCode\nndvi_spring.getInfo()\n\n\n\n            \n            \n\n\n{'type': 'Image',\n 'bands': [{'id': 'NDVI',\n   'data_type': {'type': 'PixelType',\n    'precision': 'float',\n    'min': -1,\n    'max': 1},\n   'crs': 'EPSG:4326',\n   'crs_transform': [1, 0, 0, 0, 1, 0]}]}\n\n\nThis code computes the Soil Adjusted Vegetation Index (SAVI) for the spring image collection. SAVI is a vegetation index similar to NDVI but incorporates a soil brightness correction factor to account for variations in soil reflectance.\nThe formula is SAVI = ((Band 5 â€“ Band 4) / (Band 5 + Band 4 + 0.5)) * (1.5)\n\n\nCode\n## Compute Soil Adjusted Vegetation Index (SAVI)\nsavi_spring = imageSpring.expression(\n        '1.5 * (NIR - RED) / (NIR + RED + 0.5)', {\n            'NIR': imageSpring.select('SR_B5'),\n            'RED': imageSpring.select('SR_B4')\n        }\n    ).rename('SAVI')\n\n\nThis code computes the Enhanced Vegetation Index (EVI) for the spring image collection. EVI is a vegetation index designed to minimize the influence of atmospheric conditions and background noise on vegetation assessments.\nThe formula is: EVI = 2.5 * ((Band 5 â€“ Band 4) / (Band 5 + 6 * Band 4 â€“ 7.5 * Band 2 + 1))\n\n\nCode\n## Calculate Enhanced Vegetation Index (EVI)\nevi_spring = imageSpring.expression(\n    '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n        'NIR': imageSpring.select('SR_B5'),  # Near-Infrared (NIR) band\n        'RED': imageSpring.select('SR_B4'),  # Red band\n        'BLUE': imageSpring.select('SR_B2')  # Blue band\n    }).rename('EVI')\n\n\nFinally, we need to calculate the land surface temperature, which is broken down into several steps. - Minimum and Maximum NDVI Calculation: It calculates the minimum and maximum NDVI values within the AOI using the reduceRegion() method. The reducer parameter specifies the type of aggregation (in this case, min() and max()) - Fraction of Vegetation (FV) Calculation: It computes the Fraction of Vegetation (FV) using the NDVI values, NDVI_min, and NDVI_max obtained in the previous step. The formula calculates the square of the normalized difference between NDVI and NDVI_min divided by the difference between NDVI_max and NDVI_min. - Emissivity (EM) Calculation: It calculates the emissivity using the FV values obtained from the previous step. The formula computes the emissivity based on the FV values according to the provided equation. - Land Surface Temperature (LST) Calculation: It computes the Land Surface Temperature (LST) using the thermal band (Band 10) from the Landsat imagery and the emissivity values calculated earlier. The formula calculates the LST based on the Planckâ€™s Law, considering the thermal band as temperature in Kelvin and the calculated emissivity.\n\n\nCode\n## Compute the Land Surface Temperature\n\n# Calculate the minimum and maximum NDVI values within the AOI\nndvi_min = ndvi_spring.reduceRegion(\n    reducer=ee.Reducer.min(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI')\n\nndvi_max = ndvi_spring.reduceRegion(\n    reducer=ee.Reducer.max(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI')\n\n# Convert NDVI_min and NDVI_max to ee.Number for operations\nndvi_min = ee.Number(ndvi_min)\nndvi_max = ee.Number(ndvi_max)\n\n# Fraction of Vegetation (FV) Calculation\n# Formula: ((NDVI - NDVI_min) / (NDVI_max - NDVI_min))^2\nfv = ndvi_spring.subtract(ndvi_min).divide(ndvi_max.subtract(ndvi_min)).pow(2).rename('FV')\n\n# Emissivity Calculation\n# Formula: 0.004 * FV + 0.986\nem = fv.multiply(0.004).add(0.986).rename('EM')\n\n# Select Thermal Band (Band 10) and Rename It\nthermal = imageSpring.select('ST_B10').rename('thermal')\n\nlst_spring = thermal.expression(\n    '(TB / (1 + (0.00115 * (TB / 1.438)) * log(em))) - 273.15',\n    {\n        'TB': thermal.select('thermal'),  # Thermal band as the temperature in Kelvin\n        'em': em  # Emissivity\n    }\n).rename('LST')\n\n\nIn addition to these indices, we will also include the bands into our model. We could get them from simply selecting bands from the spring image series.\n\n\nCode\n## Get Individual Band Information\nband1_spring = imageSpring.select('SR_B1').rename('B1')\nband2_spring = imageSpring.select('SR_B2').rename('B2')\nband3_spring = imageSpring.select('SR_B3').rename('B3')\nband4_spring = imageSpring.select('SR_B4').rename('B4')\nband5_spring = imageSpring.select('SR_B5').rename('B5')\nband6_spring = imageSpring.select('SR_B6').rename('B6')\nband7_spring = imageSpring.select('SR_B7').rename('B7')\n\n\nWe summarized remote sensing data into the fishnet using zonal statistics. This is one of the easiest method of brining raster information into vector geometries. In Earth Engine, zonal statistics can be calculated by aggregating pixel values within geometries defined by feature collections (in our case, the fishnet). Common summary statistics computed for each zone include mean, sum, minimum, maximum, standard deviation, and percentile values (in our case, we will calculate the mean).\n\n\nCode\n## Helper Function - Zonal Statistics\ndef export_zonal_stats(image, reducer, index_name):\n    zonal_stats = image.reduceRegions(\n        collection=pa_fishnet_ee,\n        reducer=reducer,\n        scale=30\n    )\n\n    task = ee.batch.Export.table.toDrive(\n        collection=zonal_stats,\n        description=index_name,\n        fileFormat='CSV',\n        folder='remote-sensing' # default to our shared google drive for the project\n    )\n    task.start()\n\n\n\n            \n            \n\n\nWe wrote the export zonal statistics function and by default, it will write the output directly to google drive by default. If you would like to reproduce our result, please directly load the results or set up connections to your own google drive and create a new folder in it named remote-sensing.\n\n\nCode\n## Apply Zonal Statistics\n#### -------- DO NOT RUN THIS CODE CHUNK IF YOU WANT TO REPRODUCE THE RESULT AS IT WILL SAVE FILES TO GOOGLE DRIVE BY DEFAULT -----------\nreducer = ee.Reducer.mean()\npa_fishnet_ee = geemap.geopandas_to_ee(pa_fishnet)\nindices = {\n    'ndvi_spring': ndvi_spring,\n    'evi_spring': evi_spring,\n    'lst_spring': lst_spring,\n    'savi_spring': savi_spring,\n    'band1_spring': band1_spring,\n    'band2_spring': band2_spring,\n    'band3_spring': band3_spring,\n    'band4_spring': band4_spring,\n    'band5_spring': band5_spring,\n    'band6_spring': band6_spring,\n    'band7_spring': band7_spring\n}\n\nfor index_name, index_image in indices.items():\n    export_zonal_stats(index_image, reducer, index_name)\n\n\n\n\nCode\n#### --------------- INSTEAD, PLEASE LOAD THE RESULTS DIRECTLY FORM OUR REPO --------------------------\nndvi_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/ndvi_spring.csv')\nevi_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/evi_spring.csv')\nsavi_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/savi_spring.csv')\nlst_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/lst_spring.csv')\nband1_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band1_spring.csv')\nband2_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band2_spring.csv')\nband3_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band3_spring.csv')\nband4_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band4_spring.csv')\nband5_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band5_spring.csv')\nband6_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band6_spring.csv')\nband7_spring_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band7_spring.csv')\n\n\nWe may see that the code above will give us 11 csv separate csv files, each with three columns. The system:index is a unique identifier for each fishnet, the mean is the mean pixels values, the .geo is the geometry of each fishnet. Our next taks is to join the 11 files together and covert it into a GeoDataFrame.\n\n\nCode\nndvi_spring_df.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\nsystem:index\nmean\n.geo\n\n\n\n\n0\n0\n0.582499\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.6210697...\n\n\n1\n1\n0.560937\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.5312382...\n\n\n2\n2\n0.613952\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.5761540...\n\n\n3\n3\n0.555680\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.4863225...\n\n\n4\n4\n0.547725\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.4414067...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThe joining of dataframes can be divided into the following parts:\n\nCreate a list of spring_dfs containing the name of the dataframes ready to be join.\nSelect a DataFrame to being the merge.\nIterates over the remaining DataFrames in the list spring_dfs and merges them into the starting DataFrame. During the merge, it renames the mean column of each DataFrame to include the index name (e.g., evi_mean, savi_mean) and combines them based on a common key (â€˜system:indexâ€™).\nSome additional editing of the final dataframe include renaming the column to include the corresponding index name, converting geometry to a shapely geometry object, and writing the GeoDataFrame.\n\n\n\nCode\n## Combine Spring Dataframes\nspring_dfs = [ndvi_spring_df,\n       evi_spring_df,\n       savi_spring_df,\n       lst_spring_df,\n       band1_spring_df,\n       band2_spring_df,\n       band3_spring_df,\n       band4_spring_df,\n       band5_spring_df,\n       band6_spring_df,\n       band7_spring_df]\n\n# starting dataframe\nspring_final = spring_dfs[0]\ndf_names = ['ndvi', 'evi', 'savi', 'lst', 'band1', 'band2', 'band3', 'band4', 'band5', 'band6', 'band7']\n\n# merge the rest of the dataframes to the starting dataframe\nfor df, name in zip(spring_dfs[1:], df_names[1:]):\n    df.drop(columns=['.geo'], inplace=True)\n    new_mean_column_name = f\"{name}_mean\"  # Form the new column name\n    df.rename(columns={'mean': new_mean_column_name}, inplace=True)\n    spring_final = pd.merge(spring_final, df, on='system:index', how='left')\n\nspring_final.rename(columns={'mean': 'ndvi_mean'}, inplace=True)\nspring_final.rename(columns={'system:index': 'NetID'}, inplace=True)\nspring_final['geometry'] = spring_final['.geo'].apply(lambda x: shape(json.loads(x)))\nspring_final_geom = gpd.GeoDataFrame(spring_final, geometry='geometry')\n\n\nLetâ€™s check the output of our merge.\n\n\nCode\n## Check Data\nspring_final.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\nNetID\nndvi_mean\n.geo\nevi_mean\nsavi_mean\nlst_mean\nband1_mean\nband2_mean\nband3_mean\nband4_mean\nband5_mean\nband6_mean\nband7_mean\ngeometry\n\n\n\n\n0\n0\n0.582499\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.6210697...\n0.317203\n0.319262\n17.863170\n0.029108\n0.034328\n0.058367\n0.061290\n0.233169\n0.214483\n0.130735\nPOLYGON ((-79.62106979445613 39.75439960369839...\n\n\n1\n1\n0.560937\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.5312382...\n0.329028\n0.331427\n23.108124\n0.031836\n0.039398\n0.067888\n0.075006\n0.260516\n0.243111\n0.149953\nPOLYGON ((-79.53123826604418 39.75439960369839...\n\n\n2\n2\n0.613952\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.5761540...\n0.327202\n0.332131\n19.105744\n0.025416\n0.029897\n0.054197\n0.054988\n0.230909\n0.201132\n0.116945\nPOLYGON ((-79.57615403025017 39.75439960369839...\n\n\n3\n3\n0.555680\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.4863225...\n0.326954\n0.329579\n21.033911\n0.030800\n0.038006\n0.064880\n0.073164\n0.258100\n0.240018\n0.149894\nPOLYGON ((-79.48632250183822 39.75439960369839...\n\n\n4\n4\n0.547725\n{\"type\":\"Polygon\",\"coordinates\":[[[-79.4414067...\n0.303537\n0.305277\n19.811644\n0.028708\n0.034169\n0.058865\n0.060915\n0.226415\n0.215652\n0.132315\nPOLYGON ((-79.44140673763222 39.75439960369839...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nWe could also visualize all the variables we have so far for using our processed spring data. We may see that the ndvi (as well as two other vegetation indices) are higher in the southeastern and southwestern part of the state but lower in the central part of the state.\n\n\nCode\n## Visualizing Variables for Spring\ncolumns_to_plot = ['savi_mean', 'ndvi_mean', 'lst_mean', 'evi_mean', 'band1_mean', 'band2_mean', 'band3_mean', 'band4_mean', 'band5_mean', 'band6_mean', 'band7_mean']\n\nnum_plots = len(columns_to_plot)\nnum_cols = 3  #\nnum_rows = num_plots // num_cols + (1 if num_plots % num_cols != 0 else 0)\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 10))\nif num_rows &gt; 1:\n    axes = axes.flatten()\n\n# Loop over the columns and plot each variable as small multiples\nfor i, column in enumerate(columns_to_plot):\n    row_index = i // num_cols\n    col_index = i % num_cols\n    spring_final_geom.plot(column=column, cmap='viridis', legend=True, ax=axes[i])\n\n    axes[i].set_title(f'{column.capitalize()}_by_fishnet')\n    axes[i].set_xlabel('Longitude')\n    axes[i].set_ylabel('Latitude')\n\n# Remove empty subplots if necessary\nfor i in range(num_plots, num_rows * num_cols):\n    fig.delaxes(axes[i])\n\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\n\n3.2 Prepare Summer Data\nNow, we will repeate the exact same processing steps for the summer data.\n\n\nCode\nimageSummer = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n            .filterBounds(aoi) \\\n            .filterDate(startSummer, endSummer) \\\n            .map(apply_scale_factors) \\\n            .map(cloud_mask) \\\n            .median() \\\n            .clip(aoi)\n\n\n\n            \n            \n\n\n\n\nCode\nndvi_summer = imageSummer.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\nsavi_summer = imageSummer.expression(\n        '1.5 * (NIR - RED) / (NIR + RED + 0.5)', {\n            'NIR': imageSummer.select('SR_B5'),\n            'RED': imageSummer.select('SR_B4')\n        }\n    ).rename('SAVI')\nevi_summer = imageSummer.expression(\n    '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n        'NIR': imageSummer.select('SR_B5'),\n        'RED': imageSummer.select('SR_B4'),\n        'BLUE': imageSummer.select('SR_B2')\n    }).rename('EVI')\nndvi_min = ndvi_summer.reduceRegion(\n    reducer=ee.Reducer.min(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI')\nndvi_max = ndvi_summer.reduceRegion(\n    reducer=ee.Reducer.max(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI')\nndvi_min = ee.Number(ndvi_min)\nndvi_max = ee.Number(ndvi_max)\nfv = ndvi_summer.subtract(ndvi_min).divide(ndvi_max.subtract(ndvi_min)).pow(2).rename('FV')\nem = fv.multiply(0.004).add(0.986).rename('EM')\nthermal = imageSummer.select('ST_B10').rename('thermal')\nlst_summer = thermal.expression(\n    '(TB / (1 + (0.00115 * (TB / 1.438)) * log(em))) - 273.15',\n    {\n        'TB': thermal.select('thermal'),\n        'em': em\n    }\n).rename('LST')\n\n\n\n            \n            \n\n\n\n\nCode\nband1_summer = imageSummer.select('SR_B1').rename('B1')\nband2_summer = imageSummer.select('SR_B2').rename('B2')\nband3_summer = imageSummer.select('SR_B3').rename('B3')\nband4_summer = imageSummer.select('SR_B4').rename('B4')\nband5_summer = imageSummer.select('SR_B5').rename('B5')\nband6_summer = imageSummer.select('SR_B6').rename('B6')\nband7_summer = imageSummer.select('SR_B7').rename('B7')\n\n\n\n            \n            \n\n\n\n\nCode\n#### -------- DO NOT RUN THIS CODE CHUNK IF YOU WANT TO REPRODUCE THE RESULT AS IT WILL SAVE FILES TO GOOGLE DRIVE BY DEFAULT -----------\nindices = {\n    'ndvi_summer': ndvi_summer,\n    'evi_summer': evi_summer,\n    'lst_summer': lst_summer,\n    'savi_summer': savi_summer,\n    'band1_summer': band1_summer,\n    'band2_summer': band2_summer,\n    'band3_summer': band3_summer,\n    'band4_summer': band4_summer,\n    'band5_summer': band5_summer,\n    'band6_summer': band6_summer,\n    'band7_summer': band7_summer\n}\n\nfor index_name, index_image in indices.items():\n    export_zonal_stats(index_image, reducer, index_name)\n\n\n\n            \n            \n\n\n\n\nCode\n#### --------------- INSTEAD, PLEASE LOAD THE RESULTS DIRECTLY FORM OUR REPO --------------------------\nndvi_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/ndvi_summer.csv')\nevi_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/evi_summer.csv')\nsavi_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/savi_summer.csv')\nlst_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/lst_summer.csv')\nband1_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band1_summer.csv')\nband2_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band2_summer.csv')\nband3_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band3_summer.csv')\nband4_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band4_summer.csv')\nband5_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band5_summer.csv')\nband6_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band6_summer.csv')\nband7_summer_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band7_summer.csv')\n\n\n\n            \n            \n\n\n\n\nCode\nsummer_dfs = [ndvi_summer_df,\n       evi_summer_df,\n       savi_summer_df,\n       lst_summer_df,\n       band1_summer_df,\n       band2_summer_df,\n       band3_summer_df,\n       band4_summer_df,\n       band5_summer_df,\n       band6_summer_df,\n       band7_summer_df]\nsummer_final = summer_dfs[0]\ndf_names = ['ndvi', 'evi', 'savi', 'lst', 'band1', 'band2', 'band3', 'band4', 'band5', 'band6', 'band7']\n\nfor df, name in zip(summer_dfs[1:], df_names[1:]):\n    df.drop(columns=['.geo'], inplace=True)\n    new_mean_column_name = f\"{name}_mean\"\n    df.rename(columns={'mean': new_mean_column_name}, inplace=True)\n    summer_final = pd.merge(summer_final, df, on='system:index', how='left')\n\nsummer_final.rename(columns={'mean': 'ndvi_mean'}, inplace=True)\nsummer_final.rename(columns={'system:index': 'NetID'}, inplace=True)\nsummer_final['geometry'] = summer_final['.geo'].apply(lambda x: shape(json.loads(x)))\nsummer_final_geom = gpd.GeoDataFrame(summer_final, geometry='geometry')\n\n\n\n\n3.3 Prepare Fall Data\nNow, we will repeate the exact same processing steps for the fall data.\n\n\nCode\nimageFall = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n            .filterBounds(aoi) \\\n            .filterDate(startFall, endFall) \\\n            .map(apply_scale_factors) \\\n            .map(cloud_mask) \\\n            .median() \\\n            .clip(aoi)\n\n\n\n            \n            \n\n\n\n\nCode\nndvi_fall = imageFall.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\nsavi_fall = imageFall.expression(\n        '1.5 * (NIR - RED) / (NIR + RED + 0.5)', {\n            'NIR': imageFall.select('SR_B5'),\n            'RED': imageFall.select('SR_B4')\n        }\n    ).rename('SAVI')\nevi_fall = imageFall.expression(\n    '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n        'NIR': imageFall.select('SR_B5'),\n        'RED': imageFall.select('SR_B4'),\n        'BLUE': imageFall.select('SR_B2')\n    }).rename('EVI')\nndvi_min = ndvi_fall.reduceRegion(\n    reducer=ee.Reducer.min(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI')\nndvi_max = ndvi_fall.reduceRegion(\n    reducer=ee.Reducer.max(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI')\nndvi_min = ee.Number(ndvi_min)\nndvi_max = ee.Number(ndvi_max)\nfv = ndvi_fall.subtract(ndvi_min).divide(ndvi_max.subtract(ndvi_min)).pow(2).rename('FV')\nem = fv.multiply(0.004).add(0.986).rename('EM')\nthermal = imageFall.select('ST_B10').rename('thermal')\nlst_fall = thermal.expression(\n    '(TB / (1 + (0.00115 * (TB / 1.438)) * log(em))) - 273.15',\n    {\n        'TB': thermal.select('thermal'),\n        'em': em\n    }\n).rename('LST')\n\n\n\n            \n            \n\n\n\n\nCode\nband1_fall = imageFall.select('SR_B1').rename('B1')\nband2_fall = imageFall.select('SR_B2').rename('B2')\nband3_fall = imageFall.select('SR_B3').rename('B3')\nband4_fall = imageFall.select('SR_B4').rename('B4')\nband5_fall = imageFall.select('SR_B5').rename('B5')\nband6_fall = imageFall.select('SR_B6').rename('B6')\nband7_fall = imageFall.select('SR_B7').rename('B7')\n\n\n\n            \n            \n\n\n\n\nCode\n#### -------- DO NOT RUN THIS CODE CHUNK IF YOU WANT TO REPRODUCE THE RESULT AS IT WILL SAVE FILES TO GOOGLE DRIVE BY DEFAULT -----------\nindices = {\n    'ndvi_fall': ndvi_fall,\n    'evi_fall': evi_fall,\n    'lst_fall': lst_fall,\n    'savi_fall': savi_fall,\n    'band1_fall': band1_fall,\n    'band2_fall': band2_fall,\n    'band3_fall': band3_fall,\n    'band4_fall': band4_fall,\n    'band5_fall': band5_fall,\n    'band6_fall': band6_fall,\n    'band7_fall': band7_fall\n}\n\nfor index_name, index_image in indices.items():\n    export_zonal_stats(index_image, reducer, index_name)\n\n\n\n            \n            \n\n\n\n\nCode\n#### --------------- INSTEAD, PLEASE LOAD THE RESULTS DIRECTLY FORM OUR REPO --------------------------\nndvi_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/ndvi_fall.csv')\nevi_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/evi_fall.csv')\nsavi_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/savi_fall.csv')\nlst_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/lst_fall.csv')\nband1_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band1_fall.csv')\nband2_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band2_fall.csv')\nband3_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band3_fall.csv')\nband4_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band4_fall.csv')\nband5_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band5_fall.csv')\nband6_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band6_fall.csv')\nband7_fall_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band7_fall.csv')\n\n\n\n            \n            \n\n\n\n\nCode\nfall_dfs = [ndvi_fall_df,\n       evi_fall_df,\n       savi_fall_df,\n       lst_fall_df,\n       band1_fall_df,\n       band2_fall_df,\n       band3_fall_df,\n       band4_fall_df,\n       band5_fall_df,\n       band6_fall_df,\n       band7_fall_df]\nfall_final = fall_dfs[0]\ndf_names = ['ndvi', 'evi', 'savi', 'lst', 'band1', 'band2', 'band3', 'band4', 'band5', 'band6', 'band7']\n\nfor df, name in zip(fall_dfs[1:], df_names[1:]):\n    df.drop(columns=['.geo'], inplace=True)\n    new_mean_column_name = f\"{name}_mean\"\n    df.rename(columns={'mean': new_mean_column_name}, inplace=True)\n    fall_final = pd.merge(fall_final, df, on='system:index', how='left')\n\nfall_final.rename(columns={'mean': 'ndvi_mean'}, inplace=True)\nfall_final.rename(columns={'system:index': 'NetID'}, inplace=True)\nfall_final['geometry'] = fall_final['.geo'].apply(lambda x: shape(json.loads(x)))\nfall_final_geom = gpd.GeoDataFrame(fall_final, geometry='geometry')\n\n\n\n            \n            \n\n\n\n\n3.4 Prepare Winter Data\nNow, we will repeate the exact same processing steps for the winter data.\n\n\nCode\nimageWinter = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n            .filterBounds(aoi) \\\n            .filterDate(startWinter, endWinter) \\\n            .map(apply_scale_factors) \\\n            .map(cloud_mask) \\\n            .median() \\\n            .clip(aoi)\n\n\n\n            \n            \n\n\n\n\nCode\nndvi_winter = imageWinter.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\nsavi_winter = imageWinter.expression(\n        '1.5 * (NIR - RED) / (NIR + RED + 0.5)', {\n            'NIR': imageWinter.select('SR_B5'),\n            'RED': imageWinter.select('SR_B4')\n        }\n    ).rename('SAVI')\nevi_winter = imageWinter.expression(\n    '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n        'NIR': imageWinter.select('SR_B5'),\n        'RED': imageWinter.select('SR_B4'),\n        'BLUE': imageWinter.select('SR_B2')\n    }).rename('EVI')\nndvi_min = ndvi_winter.reduceRegion(\n    reducer=ee.Reducer.min(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI')\nndvi_max = ndvi_winter.reduceRegion(\n    reducer=ee.Reducer.max(),\n    geometry=aoi,\n    scale=30,\n    maxPixels=1e9\n).get('NDVI')\nndvi_min = ee.Number(ndvi_min)\nndvi_max = ee.Number(ndvi_max)\nfv = ndvi_winter.subtract(ndvi_min).divide(ndvi_max.subtract(ndvi_min)).pow(2).rename('FV')\nem = fv.multiply(0.004).add(0.986).rename('EM')\nthermal = imageWinter.select('ST_B10').rename('thermal')\nlst_winter = thermal.expression(\n    '(TB / (1 + (0.00115 * (TB / 1.438)) * log(em))) - 273.15',\n    {\n        'TB': thermal.select('thermal'),\n        'em': em\n    }\n).rename('LST')\n\n\n\n            \n            \n\n\n\n\nCode\nband1_winter = imageWinter.select('SR_B1').rename('B1')\nband2_winter = imageWinter.select('SR_B2').rename('B2')\nband3_winter = imageWinter.select('SR_B3').rename('B3')\nband4_winter = imageWinter.select('SR_B4').rename('B4')\nband5_winter = imageWinter.select('SR_B5').rename('B5')\nband6_winter = imageWinter.select('SR_B6').rename('B6')\nband7_winter = imageWinter.select('SR_B7').rename('B7')\n\n\n\n            \n            \n\n\n\n\nCode\n#### -------- DO NOT RUN THIS CODE CHUNK IF YOU WANT TO REPRODUCE THE RESULT AS IT WILL SAVE FILES TO GOOGLE DRIVE BY DEFAULT -----------\nindices = {\n    'ndvi_winter': ndvi_winter,\n    'evi_winter': evi_winter,\n    'lst_winter': lst_winter,\n    'savi_winter': savi_winter,\n    'band1_winter': band1_winter,\n    'band2_winter': band2_winter,\n    'band3_winter': band3_winter,\n    'band4_winter': band4_winter,\n    'band5_winter': band5_winter,\n    'band6_winter': band6_winter,\n    'band7_winter': band7_winter\n}\n\nfor index_name, index_image in indices.items():\n    export_zonal_stats(index_image, reducer, index_name)\n\n\n\n\nCode\n#### --------------- INSTEAD, PLEASE LOAD THE RESULTS DIRECTLY FORM OUR REPO --------------------------\nndvi_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/ndvi_winter.csv')\nevi_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/evi_winter.csv')\nsavi_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/savi_winter.csv')\nlst_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/lst_winter.csv')\nband1_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band1_winter.csv')\nband2_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band2_winter.csv')\nband3_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band3_winter.csv')\nband4_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band4_winter.csv')\nband5_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band5_winter.csv')\nband6_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band6_winter.csv')\nband7_winter_df = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/band7_winter.csv')\n\n\n\n            \n            \n\n\n\n\nCode\nwinter_dfs = [ndvi_winter_df,\n       evi_winter_df,\n       savi_winter_df,\n       lst_winter_df,\n       band1_winter_df,\n       band2_winter_df,\n       band3_winter_df,\n       band4_winter_df,\n       band5_winter_df,\n       band6_winter_df,\n       band7_winter_df]\nwinter_final = winter_dfs[0]\ndf_names = ['ndvi', 'evi', 'savi', 'lst', 'band1', 'band2', 'band3', 'band4', 'band5', 'band6', 'band7']\n\nfor df, name in zip(winter_dfs[1:], df_names[1:]):\n    df.drop(columns=['.geo'], inplace=True)\n    new_mean_column_name = f\"{name}_mean\"\n    df.rename(columns={'mean': new_mean_column_name}, inplace=True)\n    winter_final = pd.merge(winter_final, df, on='system:index', how='left')\n\nwinter_final.rename(columns={'mean': 'ndvi_mean'}, inplace=True)\nwinter_final.rename(columns={'system:index': 'NetID'}, inplace=True)\nwinter_final['geometry'] = winter_final['.geo'].apply(lambda x: shape(json.loads(x)))\nwinter_final_geom = gpd.GeoDataFrame(winter_final, geometry='geometry')\n\n\n\n            \n            \n\n\n\n\nLoad Processed Remote Sensing Data Below (Optional)\nAlternatively, you may load the final products from this section directly from our Github repository.\n\n\nCode\n## PLAIN TABULAR DATA\nspring_final = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/spring_final.csv')\nsummer_final = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/summer_final.csv')\nfall_final = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/fall_final.csv')\nwinter_final = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/winter_final.csv')\n\n## GEOJSON DATA\nspring_final_geom = gpd.read_file('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/spring_final_geom.geojson')\nsummer_final_geom = gpd.read_file('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/summer_final_geom.geojson')\nfall_final_geom = gpd.read_file('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/winter_final_geom.geojson')\nwinter_final_geom = gpd.read_file('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/winter_final_geom.geojson')"
  },
  {
    "objectID": "projects/PA-CRD-2024.html#hospital-discharge-data",
    "href": "projects/PA-CRD-2024.html#hospital-discharge-data",
    "title": "Spatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures - An Approach Using Earth-Engine-Based Remote Sensing Data and Air Quality Variables in the State of Pennsylvania",
    "section": "4. Hospital Discharge Data",
    "text": "4. Hospital Discharge Data\nWe downloaded the hospital discharge data from the Pennsylvania Department of Healthâ€™s data dashboard. The Enterprise Data Dissemination Informatics Exchange (EDDIE) is an interactive health statistics dissemination web tool where user create customized data tables, charts and maps for various health related data. We obtained hospitalization discharge due to chronic lower respiratory diseases only, of which the age-adjusted rates are standardized to the year 2000 US million population.\nSince the raw hospital data is a plain CSV file, we need to first append county geometries to it by joining it with our PA geometries file, using county name as the match.\n\n\nCode\n## Load Hospital Data\nhospital_raw = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/raw/HospitalizationDischarges.csv')\nhospital_clean = hospital_raw[['Geography', 'Count', 'Geography_Code', 'Population']]\nhospital_clean['Geography'] = hospital_clean['Geography'].str.lower()  # Convert values to lowercase\n\n\n\n            \n            \n\n\nWe may see that the raw hospital data does not contain any geometries, but the county name can be helpful for us to join it to the county geometries.\n\n\nCode\nhospital_clean.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\nGeography\nCount\nGeography_Code\nPopulation\n\n\n\n\n0\npennsylvania\n21,076\n0\n12,972,008\n\n\n1\nadams\n110\n1\n106,027\n\n\n2\nallegheny\n1,664\n3\n1,233,253\n\n\n3\narmstrong\n99\n5\n64,747\n\n\n4\nbeaver\n238\n7\n165,677\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nCode\n## Clean State Boundary Data\npa_clean = pa[['fips_count', 'county_nam', 'geometry']]\npa_clean['Geography'] = pa_clean['county_nam'].str.lower()  # Convert values to lowercase\npa_clean = pa_clean.drop(columns=['county_nam'])\n\n\n\n\nCode\ncounty_discharge = pd.merge(pa_clean, hospital_clean, on='Geography', how='left')\n\n\nAfter the join, the county_discharge data will look like the following.\n\n\nCode\ncounty_discharge.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\nfips_count\ngeometry\nGeography\nCount\nGeography_Code\nPopulation\n\n\n\n\n0\n091\nMULTIPOLYGON (((-75.04927 40.11219, -75.05431 ...\nmontgomery\n1,095\n91\n864,683\n\n\n1\n015\nMULTIPOLYGON (((-76.14569 41.99886, -76.14364 ...\nbradford\n127\n15\n59,866\n\n\n2\n017\nMULTIPOLYGON (((-75.19130 40.58571, -75.19157 ...\nbucks\n963\n17\n645,054\n\n\n3\n117\nMULTIPOLYGON (((-76.91122 41.58167, -76.91133 ...\ntioga\n63\n117\n41,106\n\n\n4\n119\nMULTIPOLYGON (((-76.91306 40.88455, -76.91356 ...\nunion\n33\n119\n42,744\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThrough visualizing the hospital discharge of CRDs in Pennsylvania, we may see significant urban-rural differences in CRD risks. CRD hospitalization is much higher in Philadelphia and surrounding counties as well as in Pittsburgh. We may also see that CRD data is missing for one county.\n\n\nCode\n## Visualize Hospital Discharge Data\nfig, axs = plt.subplots(1, 2, figsize=(16,5))\n\nmap = county_discharge.copy()\n\nmap['Population'] = map['Population'].str.replace(',', '')\nmap['Population'] = pd.to_numeric(map['Population'], errors='coerce')\nmap['Count'] = map['Count'].str.replace(',', '')\nmap['Count'] = pd.to_numeric(map['Count'], errors='coerce')\n\n\nmap.plot(column='Population', cmap='viridis', linewidth=0.8, ax=axs[0], edgecolor='0.8', legend=True)\naxs[0].set_title('Population by County')\n\nmap.plot(column='Count', cmap='viridis', linewidth=0.8, ax=axs[1], edgecolor='0.8', legend=True)\naxs[1].set_title('CRD Count by County')\n\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\n4.1 Aggregate Hospital Data to Fishnet\nHaving the hospital discharge data at the county is not very useful as our desired spatial unit of analysis is the fishnet. The workflow weâ€™ve designed to aggregate hospital data to the fishnet is to first create a NetID (starting from zero) for each fishnet. Then, we create a centroid for each fishnet (fishpoint). Depending on which county each centroid falls into, we will append the name of the county and the corresponding hospital discharge to that fishnet grid. For visuaization purpose, we will join the fishpoint layer back to the fishnet polygon by the NetID.\n\n\nCode\n## Prepare Fishpoint\nid = len(pa_fishnet.index)\npa_fishnet['id'] = range(0, id)\nfishpoint = pa_fishnet.copy()\nfishpoint['centroid'] = fishpoint.geometry.centroid\nfishpoint.drop(columns=['geometry'], inplace=True)\nfishpoint.rename(columns={'centroid': 'geometry'}, inplace=True)\n\n\n\n            \n            \n\n\n\n\nCode\n# if points fall within a county, give the name of the county, join with it\nfishpoint_discharge = gpd.sjoin(fishpoint, county_discharge, how='left', op='intersects')\n# bring points back to net\nfishpoint_discharge.drop(columns=['geometry'], inplace=True)\nfishnet_discharge = pd.merge(pa_fishnet, fishpoint_discharge, on='id', how='left')\n\n\n\n            \n            \n\n\nDue to some the geometry errors of the Pennsylvanian state layer, there are in total five fishnet grids that failed to join with the county layer and hence recieve no CRD data. We will drop these entries before running the model.\n\n\nCode\n# Find the rows where the \"Count\" column is null\nnull_rows = fishnet_discharge[fishnet_discharge['Count'].isnull()]\nprint(null_rows)\n\n\n\n            \n            \n\n\n                                               geometry    id  index_right  \\\n4326  MULTIPOLYGON (((-80.11514 42.16226, -80.07840 ...  4326          NaN   \n5926  MULTIPOLYGON (((-75.03966 40.40934, -75.03964 ...  5926          NaN   \n6150  MULTIPOLYGON (((-75.08458 40.82862, -75.08452 ...  6150          NaN   \n7283  POLYGON ((-74.99475 41.12175, -74.95051 41.121...  7283          NaN   \n7988  MULTIPOLYGON (((-75.26424 41.86927, -75.26421 ...  7988          NaN   \n\n     fips_count Geography Count  Geography_Code Population  \n4326        NaN       NaN   NaN             NaN        NaN  \n5926        NaN       NaN   NaN             NaN        NaN  \n6150        NaN       NaN   NaN             NaN        NaN  \n7283        NaN       NaN   NaN             NaN        NaN  \n7988        NaN       NaN   NaN             NaN        NaN  \n\n\nAn additional step here is to normalize/scale the count of CRDs in each county to its total population. This can be achieved through calculating the CRD rate per thousand population.\n\n\nCode\nfishnet_discharge['Count'] = fishnet_discharge['Count'].str.replace(',', '')\nfishnet_discharge['Count'] = pd.to_numeric(fishnet_discharge['Count'], errors='coerce')\nfishnet_discharge['Population'] = fishnet_discharge['Population'].str.replace(',', '')\nfishnet_discharge['Population'] = pd.to_numeric(fishnet_discharge['Population'], errors='coerce')\nfishnet_discharge['CRD_Rate'] = fishnet_discharge['Count'] / fishnet_discharge['Population'] * 1000\n\n\n\n            \n            \n\n\nIf we visualize hospitalization discharge of patient with CRD rate per thousand people this time instead of of the raw CRD count, we may see more variations in CRDs across different counties. A few counties in the northern and central part of Pennsylvania stand out as having higher CRD rate.\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize=(12, 5))\nfishnet_discharge.plot(column='CRD_Rate', cmap='viridis', linewidth=0.4, ax=ax, edgecolor='0.8', legend=True)\nplt.title('CRD Rate by County')\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\n\n4.2 Combine Hospital Data and Remote Sensing Data\nWe also need to combine hospital discharge with remote sensing data. This can be easily done with a simple join because we have created a NetID for both the remote sensing datasets and our fishnet that now contains hospital discharge data. To further makesure the IDs will match, we selected the first 400 fishnet grids from both datasets and have found the output to match each other. This means we can go for the join with no worry.\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\nfiltered_net = fishnet_discharge.head(400)\nfiltered_spring = spring_final_geom.head(400)\n\n# Plot the filtered spring DataFrame in the first subplot\nfiltered_spring.plot(ax=axs[0], cmap='viridis')\naxs[0].set_title('First 400 Row of Remote Sensing Data by Fishnet')\n\n# Plot the filtered net DataFrame in the second subplot\nfiltered_net.plot(ax=axs[1], cmap='viridis')\naxs[1].set_title('First 400 Row of Hospital Discharge by Fishnet')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nWe will repeate the join four times, each for different season. We will also drop all grids without hospital discharge data. This includes the five grids that are not joined to the county as well as the entire county without CRD data. This gives us 8309 grids in total. Originally, there were 8397 grids.\n\n\nCode\nspring_discharge_bynet = pd.merge(fishnet_discharge, spring_final, left_on='id', right_on='NetID', how='left')\nspring_discharge_bynet.dropna(subset=['CRD_Rate'], inplace=True)\nsummer_discharge_bynet = pd.merge(fishnet_discharge, summer_final, left_on='id', right_on='NetID', how='left')\nsummer_discharge_bynet.dropna(subset=['CRD_Rate'], inplace=True)\nfall_discharge_bynet = pd.merge(fishnet_discharge, fall_final, left_on='id', right_on='NetID', how='left')\nfall_discharge_bynet.dropna(subset=['CRD_Rate'], inplace=True)\nwinter_discharge_bynet = pd.merge(fishnet_discharge, winter_final, left_on='id', right_on='NetID', how='left')\nwinter_discharge_bynet.dropna(subset=['CRD_Rate'], inplace=True)\n\n\n\n            \n            \n\n\nUsing the spring data as an exmaple, the joined dataset should look like the following.\n\n\nCode\nspring_discharge_bynet.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\ngeometry_x\nid\nindex_right\nfips_count\nGeography\nCount\nGeography_Code\nPopulation\nCRD_Rate\nNetID\n...\nsavi_mean\nlst_mean\nband1_mean\nband2_mean\nband3_mean\nband4_mean\nband5_mean\nband6_mean\nband7_mean\ngeometry_y\n\n\n\n\n0\nPOLYGON ((-79.62107 39.75440, -79.57615 39.754...\n0\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n0\n...\n0.319262\n17.863170\n0.029108\n0.034328\n0.058367\n0.061290\n0.233169\n0.214483\n0.130735\nPOLYGON ((-79.62106979445613 39.75439960369839...\n\n\n1\nPOLYGON ((-79.53124 39.75440, -79.48632 39.754...\n1\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n1\n...\n0.331427\n23.108124\n0.031836\n0.039398\n0.067888\n0.075006\n0.260516\n0.243111\n0.149953\nPOLYGON ((-79.53123826604418 39.75439960369839...\n\n\n2\nPOLYGON ((-79.57615 39.75440, -79.53124 39.754...\n2\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n2\n...\n0.332131\n19.105744\n0.025416\n0.029897\n0.054197\n0.054988\n0.230909\n0.201132\n0.116945\nPOLYGON ((-79.57615403025017 39.75439960369839...\n\n\n3\nPOLYGON ((-79.48632 39.75440, -79.44141 39.754...\n3\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n3\n...\n0.329579\n21.033911\n0.030800\n0.038006\n0.064880\n0.073164\n0.258100\n0.240018\n0.149894\nPOLYGON ((-79.48632250183822 39.75439960369839...\n\n\n4\nPOLYGON ((-79.44141 39.78892, -79.39649 39.788...\n4\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n4\n...\n0.305277\n19.811644\n0.028708\n0.034169\n0.058865\n0.060915\n0.226415\n0.215652\n0.132315\nPOLYGON ((-79.44140673763222 39.75439960369839...\n\n\n\n\n5 rows Ã— 23 columns"
  },
  {
    "objectID": "projects/PA-CRD-2024.html#air-quality-data",
    "href": "projects/PA-CRD-2024.html#air-quality-data",
    "title": "Spatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures - An Approach Using Earth-Engine-Based Remote Sensing Data and Air Quality Variables in the State of Pennsylvania",
    "section": "5. Air Quality Data",
    "text": "5. Air Quality Data\nAir quality data are usually collected through field measurements. In our project, this field data were obtained from the Ambient Air Monitoring Data Reports - Monthly Parameter Detail Table. This is an officia app of the PA government that collects and calcualtes the mean of a variety of air quality indices by month. Because this app does not provide the location of these measuring sites, we obtained the location of measuring sites from this interactive map.\nOne of the biggest challenges of processing the air quality data is that different air quality indices are collected by different stations. For example, one station collects PM2.5 and O3 but not NO2 and SO2, while the other station collects wind speed and solar radiation data. This means that we have to make unique spatial interpolation of the influence zone of each air quality indices, based on the location of the specific measuring sites that collec them. Therefore, for each kind of air quality indices that we collect, we computed their voronoi polygon respectively.\nIn the original paper, the authors have collected Wind speed (WS), Wind direction (WD), Precipitation (PR), carbon oxide (CO), PM2.5, PM10, SO2, O3, and NO2 data at each station, but in the state Pennsylvania, air quality data is much less complete. This leads to our second biggest challenges, determining the air quality indices to collect. After carefully examining the completeness of data, we eventually selected PM2.5, O3, Solar Radiation, and Wind Speed as indices to include. These are the top four indices with the most number of stations collect them in Pennsylvania. The more stations collecting these data, the more accurate our spatial interpolation will be because our voronoi polygon will better capture the intra-regional variations in air quality measures. There are some occurences of missing data, but this will not significantly impact our analysis.\nWith these in mind, we downloaded the monthly average measures of these four indices for each of their stations, average them by season, and join the locations of these stations to the dataframe.\n\n5.1 Process and Aggregate PM2.5 Data\nWe begin processing the air quality data by first constructing geometries for it based on the longitude and latitude of the station locations. After that, we need to make sure it has the same projection as our other datasets.\n\n\nCode\npm25_raw = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/raw/AQ-Data-PM25.csv')\npm25_geom = gpd.GeoDataFrame(pm25_raw, geometry=gpd.points_from_xy(pm25_raw['Longitude'], pm25_raw['Latitude']))\npm25_geom.crs = {'init': 'epsg:4326'}\n\n\nBelows is the helper function that takes a Voronoi diagram in 2D as input and reconstructs the infinite Voronoi regions into finite regions. The function is adapted from this GitHub post.\nIt can be broken down to the following components, as documented by the function author:\n\nInput Validation: It checks if the input Voronoi diagram is in 2D.\nInitialization: It initializes empty lists to store the new regions and vertices.\nCenter and Radius Calculation: It calculates the center of the diagram and, if not provided, determines a suitable radius based on the maximum point-to-point distance.\nRidge Mapping: It constructs a map that contains all ridges for each point in the diagram.\nRegion Reconstruction: It iterates through each point in the original Voronoi diagram and its corresponding region.\nHandling Finite Regions: If a region is finite (i.e., all vertices have non-negative indices), it adds the vertices directly to the new regions.\nHandling Non-finite Regions: If a region is non-finite (i.e., contains vertices with negative indices indicating they are part of infinite ridges), it reconstructs the missing vertices at infinity.\nComputing Infinite Ridge Endpoints: It computes the missing endpoint of each infinite ridge by determining the direction from the midpoint of the ridge towards the outer boundary, then extending it by the specified radius. Sorting Vertices: It sorts the vertices of each region in counterclockwise order.\nFinishing Up: It appends the reconstructed region to the list of new regions. Return: It returns the lists of revised regions and vertices.\n\nUltimately, this function reconstructs infinite Voronoi regions into finite ones, effectively handling edge cases where the diagram extends infinitely. The function returns regions, which contains the indices of the vertices in each revised Voronoi region, and vertices, which contains the coordinates for the revised Voronoi vertices.\n\n\nCode\ndef voronoi_finite_polygons_2d(vor, radius=None):\n    \"\"\"\n    Reconstruct infinite voronoi regions in a 2D diagram to finite\n    regions.\n\n    Parameters\n    ----------\n    vor : Voronoi\n        Input diagram\n    radius : float, optional\n        Distance to 'points at infinity'.\n\n    Returns\n    -------\n    regions : list of tuples\n        Indices of vertices in each revised Voronoi regions.\n    vertices : list of tuples\n        Coordinates for revised Voronoi vertices. Same as coordinates\n        of input vertices, with 'points at infinity' appended to the\n        end.\n\n    \"\"\"\n\n    if vor.points.shape[1] != 2:\n        raise ValueError(\"Requires 2D input\")\n\n    new_regions = []\n    new_vertices = vor.vertices.tolist()\n\n    center = vor.points.mean(axis=0)\n    if radius is None:\n        radius = vor.points.ptp().max()\n\n    # Construct a map containing all ridges for a given point\n    all_ridges = {}\n    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n\n    # Reconstruct infinite regions\n    for p1, region in enumerate(vor.point_region):\n        vertices = vor.regions[region]\n\n        if all(v &gt;= 0 for v in vertices):\n            # finite region\n            new_regions.append(vertices)\n            continue\n\n        # reconstruct a non-finite region\n        ridges = all_ridges[p1]\n        new_region = [v for v in vertices if v &gt;= 0]\n\n        for p2, v1, v2 in ridges:\n            if v2 &lt; 0:\n                v1, v2 = v2, v1\n            if v1 &gt;= 0:\n                # finite ridge: already in the region\n                continue\n\n            # Compute the missing endpoint of an infinite ridge\n\n            t = vor.points[p2] - vor.points[p1] # tangent\n            t /= np.linalg.norm(t)\n            n = np.array([-t[1], t[0]])  # normal\n\n            midpoint = vor.points[[p1, p2]].mean(axis=0)\n            direction = np.sign(np.dot(midpoint - center, n)) * n\n            far_point = vor.vertices[v2] + direction * radius\n\n            new_region.append(len(new_vertices))\n            new_vertices.append(far_point.tolist())\n\n        # sort region counterclockwise\n        vs = np.asarray([new_vertices[v] for v in new_region])\n        c = vs.mean(axis=0)\n        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n        new_region = np.array(new_region)[np.argsort(angles)]\n\n        # finish\n        new_regions.append(new_region.tolist())\n\n    return new_regions, np.asarray(new_vertices)\n\n\nWe took the point coordinates of our PM2.5 dataframe, and generates a Voronoi diagram vor_pm25. The Voronoi diagram creates regions where each point in a region is closer to its associated data point than to any other data point. Then our function reconstructs infinite Voronoi regions into finite ones.\n\n\nCode\npm25_coords = np.column_stack((pm25_geom.geometry.x, pm25_geom.geometry.y))\nvor_pm25 = Voronoi(pm25_coords)\nregions_pm25, vertices_pm25 = voronoi_finite_polygons_2d(vor_pm25)\n\n\nBelow is a quick visualization of the voronoi polygons of PM2.5 measuring stations in Pennsylvania.\n\n\nCode\nfor region in regions_pm25:\n    polygon = vertices_pm25[region]\n    plt.fill(*zip(*polygon), alpha=0.4)\n\nplt.plot(pm25_coords[:,0], pm25_coords[:,1], 'ko')\nplt.xlim(vor_pm25.min_bound[0] - 0.1, vor_pm25.max_bound[0] + 0.1)\nplt.ylim(vor_pm25.min_bound[1] - 0.1, vor_pm25.max_bound[1] + 0.1)\n\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nWe need to clip our voronoi polygons to the spatial extent of Pennsylvania. This can be achieved by iterating through the voronoi regions and then for each region, retrieves the corresponding vertices and constructs a polygon using these vertices. Then, clip each polygon with PA boundary. If the clipped polygon resulting from the intersection is empty, indicating no overlap with the boundary, it skips to the next iteration.\n\n\nCode\nclipped_voronoi_polygons = []\nfor region in regions_pm25:\n    polygon = vertices_pm25[region]\n    poly = Polygon(polygon)\n    clipped_poly = poly.intersection(pa_bound.geometry.unary_union)\n    if clipped_poly.is_empty:\n        continue\n    clipped_voronoi_polygons.append(clipped_poly)\n\n# Create a GeoDataFrame from the list of clipped Voronoi polygons\nvoronoi_pm25 = gpd.GeoDataFrame(geometry=clipped_voronoi_polygons, crs=pa_bound.crs)\n\n\n\n\nCode\nvoronoi_pm25.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOLYGON ((-75.46131 41.04261, -75.34256 40.369...\n\n\n1\nPOLYGON ((-78.65090 40.44175, -78.59594 41.517...\n\n\n2\nPOLYGON ((-76.98808 39.72000, -76.99506 39.719...\n\n\n3\nPOLYGON ((-79.97194 41.13585, -79.94506 40.831...\n\n\n4\nPOLYGON ((-77.74339 40.27050, -77.35083 40.750...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nBecause the output dataframe from the clipping operation only contains the geometry of the polygon, we need to join station information to it. This can be achieved through making an intersection between the polygons and the point of the stations.\n\n\nCode\nvoronoi_pm25_geom = gpd.sjoin(voronoi_pm25,pm25_geom, op='intersects')\nvoronoi_pm25_geom.drop(columns=['index_right', 'Latitude', 'Longitude'], inplace=True)\n\n\n\n\nCode\nvoronoi_pm25_geom.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\ngeometry\nStationName\nCountyName\nSpring\nSummer\nFall\nWinter\n\n\n\n\n0\nPOLYGON ((-75.46131 41.04261, -75.34256 40.369...\nAllentown\nLehigh\n7.27\n8.07\n7.07\n9.80\n\n\n1\nPOLYGON ((-78.65090 40.44175, -78.59594 41.517...\nAltoona\nBlair\n7.07\n8.03\n7.53\n9.67\n\n\n2\nPOLYGON ((-76.98808 39.72000, -76.99506 39.719...\nArendtsville\nAdams\n7.57\n8.73\n7.50\n9.80\n\n\n3\nPOLYGON ((-79.97194 41.13585, -79.94506 40.831...\nBeaver Falls\nBeaver\n8.50\n10.83\n9.67\n11.40\n\n\n4\nPOLYGON ((-77.74339 40.27050, -77.35083 40.750...\nCarlisle\nCumberland\n7.63\n8.47\n8.30\n11.47\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThe following code joins the fishpoint to voronoi polygon if the point intersect with the polygon. This is followed by brining the fishpoint (now with PM2.5 information) to the fishnet polygons.\n\n\nCode\nfishpoint_vor_pm25 = gpd.sjoin(fishpoint, voronoi_pm25_geom, how='left', op='intersects')\nfishpoint_vor_pm25.drop(columns=['geometry'], inplace=True)\nfishnet_pm25 = pd.merge(pa_fishnet, fishpoint_vor_pm25, on='id', how='left')\n\n\nNow that the data is at the scale of fishnet, letâ€™s join it with the remote sensing data. We will repeate that for each season. Some additional processing steps, such as renaming and dropping columns, are needed for the successful join.\n\n\nCode\nspring_discharge_bynet = pd.merge(spring_discharge_bynet, fishnet_pm25[['id', 'Spring']], left_on='id', right_on='id', how='left')\nsummer_discharge_bynet = pd.merge(summer_discharge_bynet, fishnet_pm25[['id', 'Summer']], left_on='id', right_on='id', how='left')\nfall_discharge_bynet = pd.merge(fall_discharge_bynet, fishnet_pm25[['id', 'Fall']], left_on='id', right_on='id', how='left')\nwinter_discharge_bynet = pd.merge(winter_discharge_bynet, fishnet_pm25[['id', 'Winter']], left_on='id', right_on='id', how='left')\n\n\n\n\nCode\nspring_discharge_bynet.rename(columns={'Spring': 'PM25'}, inplace=True)\nsummer_discharge_bynet.rename(columns={'Summer': 'PM25'}, inplace=True)\nfall_discharge_bynet.rename(columns={'Fall': 'PM25'}, inplace=True)\nwinter_discharge_bynet.rename(columns={'Winter': 'PM25'}, inplace=True)\n\n\n\n\nCode\nspring_discharge_bynet.drop(columns=['geometry_y'], inplace=True)\nsummer_discharge_bynet.drop(columns=['geometry_y'], inplace=True)\nfall_discharge_bynet.drop(columns=['geometry_y'], inplace=True)\nwinter_discharge_bynet.drop(columns=['geometry_y'], inplace=True)\n\n\nAs an example, this is how our joined dataset should look like at this point.\n\n\nCode\nspring_discharge_bynet.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\ngeometry_x\nid\nindex_right\nfips_count\nGeography\nCount\nGeography_Code\nPopulation\nCRD_Rate\nNetID\n...\nsavi_mean\nlst_mean\nband1_mean\nband2_mean\nband3_mean\nband4_mean\nband5_mean\nband6_mean\nband7_mean\nPM25\n\n\n\n\n0\nPOLYGON ((-79.62107 39.75440, -79.57615 39.754...\n0\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n0\n...\n0.319262\n17.863170\n0.029108\n0.034328\n0.058367\n0.061290\n0.233169\n0.214483\n0.130735\n7.0\n\n\n1\nPOLYGON ((-79.53124 39.75440, -79.48632 39.754...\n1\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n1\n...\n0.331427\n23.108124\n0.031836\n0.039398\n0.067888\n0.075006\n0.260516\n0.243111\n0.149953\n7.0\n\n\n2\nPOLYGON ((-79.57615 39.75440, -79.53124 39.754...\n2\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n2\n...\n0.332131\n19.105744\n0.025416\n0.029897\n0.054197\n0.054988\n0.230909\n0.201132\n0.116945\n7.0\n\n\n3\nPOLYGON ((-79.48632 39.75440, -79.44141 39.754...\n3\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n3\n...\n0.329579\n21.033911\n0.030800\n0.038006\n0.064880\n0.073164\n0.258100\n0.240018\n0.149894\n7.0\n\n\n4\nPOLYGON ((-79.44141 39.78892, -79.39649 39.788...\n4\n56.0\n051\nfayette\n321.0\n51.0\n125755.0\n2.552582\n4\n...\n0.305277\n19.811644\n0.028708\n0.034169\n0.058865\n0.060915\n0.226415\n0.215652\n0.132315\n7.0\n\n\n\n\n5 rows Ã— 23 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThe visualization belows shows the voronoi polygon, the fishnet, and the PM2.5 level for each season in Pennsylvania. We may see that PM2.5 tends to be higher in winter and summer while lower in spring and fall. Some counties in particular, stand out as having much higher PM2.5 than others.\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(11, 5))\n\nspring_discharge_bynet.rename(columns={'geometry_x': 'geometry'}, inplace=True)\nsummer_discharge_bynet.rename(columns={'geometry_x': 'geometry'}, inplace=True)\nfall_discharge_bynet.rename(columns={'geometry_x': 'geometry'}, inplace=True)\nwinter_discharge_bynet.rename(columns={'geometry_x': 'geometry'}, inplace=True)\n\n# Plot PM2.5 levels at fishnet for each subplot\nspring_discharge_bynet.plot(column='PM25', cmap='viridis', linewidth=0.4, ax=axes[0, 0], edgecolor='0.8', legend=True)\nsummer_discharge_bynet.plot(column='PM25', cmap='viridis', linewidth=0.4, ax=axes[0, 1], edgecolor='0.8', legend=True)\nfall_discharge_bynet.plot(column='PM25', cmap='viridis', linewidth=0.4, ax=axes[1, 0], edgecolor='0.8', legend=True)\nwinter_discharge_bynet.plot(column='PM25', cmap='viridis', linewidth=0.4, ax=axes[1, 1], edgecolor='0.8', legend=True)\n\n# Set titles for each subplot\naxes[0, 0].set_title('Spring PM2.5')\naxes[0, 1].set_title('Summer PM2.5')\naxes[1, 0].set_title('Fall PM2.5')\naxes[1, 1].set_title('Winter PM2.5')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\n\n5.2 Process and Aggregate O3 Data\nWe will repeate the exact same process to read and process the O3 data. Note that we have to construct separate voronoi polygons for O3 because the stations collecting O3 data is different from those collecting PM2.5\n\n\nCode\no3_raw = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/raw/AQ-Data-O3.csv')\no3_geom = gpd.GeoDataFrame(o3_raw, geometry=gpd.points_from_xy(o3_raw['Longitude'], o3_raw['Latitude']))\no3_geom.crs = {'init': 'epsg:4326'}\n\n\n\n\nCode\no3_coords = np.column_stack((o3_geom.geometry.x, o3_geom.geometry.y))\nvor_o3 = Voronoi(o3_coords)\nregions_o3, vertices_o3 = voronoi_finite_polygons_2d(vor_o3)\n\n\n\n\nCode\nclipped_voronoi_polygons = []\nfor region in regions_o3:\n    polygon = vertices_o3[region]\n    poly = Polygon(polygon)\n    clipped_poly = poly.intersection(pa_bound.geometry.unary_union)\n    if clipped_poly.is_empty:\n        continue\n    clipped_voronoi_polygons.append(clipped_poly)\n\nvoronoi_o3 = gpd.GeoDataFrame(geometry=clipped_voronoi_polygons, crs=pa_bound.crs)\n\n\n\n\nCode\nvoronoi_o3_geom = gpd.sjoin(voronoi_o3,o3_geom, op='intersects')\nvoronoi_o3_geom.drop(columns=['index_right', 'Latitude', 'Longitude'], inplace=True)\n\n\n\n\nCode\nfishpoint_vor_o3 = gpd.sjoin(fishpoint, voronoi_o3_geom, how='left', op='intersects')\nfishpoint_vor_o3.drop(columns=['geometry'], inplace=True)\nfishnet_o3 = pd.merge(pa_fishnet, fishpoint_vor_o3, on='id', how='left')\n\n\n\n            \n            \n\n\n\n\nCode\nspring_discharge_bynet = pd.merge(spring_discharge_bynet, fishnet_o3[['id', 'Spring']], left_on='id', right_on='id', how='left')\nsummer_discharge_bynet = pd.merge(summer_discharge_bynet, fishnet_o3[['id', 'Summer']], left_on='id', right_on='id', how='left')\nfall_discharge_bynet = pd.merge(fall_discharge_bynet, fishnet_o3[['id', 'Fall']], left_on='id', right_on='id', how='left')\nwinter_discharge_bynet = pd.merge(winter_discharge_bynet, fishnet_o3[['id', 'Winter']], left_on='id', right_on='id', how='left')\n\n\n\n            \n            \n\n\n\n\nCode\nspring_discharge_bynet.rename(columns={'Spring': 'O3'}, inplace=True)\nsummer_discharge_bynet.rename(columns={'Summer': 'O3'}, inplace=True)\nfall_discharge_bynet.rename(columns={'Fall': 'O3'}, inplace=True)\nwinter_discharge_bynet.rename(columns={'Winter': 'O3'}, inplace=True)\n\n\n\n            \n            \n\n\n\n\n5.3 Process and Aggregate Wind Speed Data\nWe will repeate the exact same process to read and process the Wind Speed data. Again, we will have to construct separate voronoi polygons for Wind Speed because the stations collecting O3 data is different from those collecting PM2.5.\n\n\nCode\nws_raw = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/raw/AQ-Data-WS.csv')\nws_geom = gpd.GeoDataFrame(ws_raw, geometry=gpd.points_from_xy(ws_raw['Longitude'], ws_raw['Latitude']))\nws_geom.crs = {'init': 'epsg:4326'}\n\n\n\n\nCode\nws_coords = np.column_stack((ws_geom.geometry.x, ws_geom.geometry.y))\nvor_ws = Voronoi(ws_coords)\nregions_ws, vertices_ws = voronoi_finite_polygons_2d(vor_ws)\n\n\n\n\nCode\nclipped_voronoi_polygons = []\nfor region in regions_ws:\n    polygon = vertices_ws[region]\n    poly = Polygon(polygon)\n    clipped_poly = poly.intersection(pa_bound.geometry.unary_union)\n    if clipped_poly.is_empty:\n        continue\n    clipped_voronoi_polygons.append(clipped_poly)\n\nvoronoi_ws = gpd.GeoDataFrame(geometry=clipped_voronoi_polygons, crs=pa_bound.crs)\n\n\n\n\nCode\nvoronoi_ws_geom = gpd.sjoin(voronoi_ws,ws_geom, op='intersects')\nvoronoi_ws_geom.drop(columns=['index_right', 'Latitude', 'Longitude'], inplace=True)\n\n\n\n\nCode\nfishpoint_vor_ws = gpd.sjoin(fishpoint, voronoi_ws_geom, how='left', op='intersects')\nfishpoint_vor_ws.drop(columns=['geometry'], inplace=True)\nfishnet_ws = pd.merge(pa_fishnet, fishpoint_vor_ws, on='id', how='left')\n\n\n\n\nCode\nspring_discharge_bynet = pd.merge(spring_discharge_bynet, fishnet_ws[['id', 'Spring']], left_on='id', right_on='id', how='left')\nsummer_discharge_bynet = pd.merge(summer_discharge_bynet, fishnet_ws[['id', 'Summer']], left_on='id', right_on='id', how='left')\nfall_discharge_bynet = pd.merge(fall_discharge_bynet, fishnet_ws[['id', 'Fall']], left_on='id', right_on='id', how='left')\nwinter_discharge_bynet = pd.merge(winter_discharge_bynet, fishnet_ws[['id', 'Winter']], left_on='id', right_on='id', how='left')\n\n\n\n\nCode\nspring_discharge_bynet.rename(columns={'Spring': 'WS'}, inplace=True)\nsummer_discharge_bynet.rename(columns={'Summer': 'WS'}, inplace=True)\nfall_discharge_bynet.rename(columns={'Fall': 'WS'}, inplace=True)\nwinter_discharge_bynet.rename(columns={'Winter': 'WS'}, inplace=True)\n\n\n\n\n5.4 Process and Aggregate Solar Radiation Data\nWe will repeate the exact same process to read and process the Solar Radiation data. We will have to reconstruct the voronoi polygon again.\n\n\nCode\nsr_raw = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/raw/AQ-Data-SR.csv')\nsr_geom = gpd.GeoDataFrame(sr_raw, geometry=gpd.points_from_xy(sr_raw['Longitude'], sr_raw['Latitude']))\nsr_geom.crs = {'init': 'epsg:4326'}\n\n\n\n\nCode\nsr_coords = np.column_stack((sr_geom.geometry.x, sr_geom.geometry.y))\nvor_sr = Voronoi(sr_coords)\nregions_sr, vertices_sr = voronoi_finite_polygons_2d(vor_sr)\n\n\n\n\nCode\nclipped_voronoi_polygons = []\nfor region in regions_sr:\n    polygon = vertices_sr[region]\n    poly = Polygon(polygon)\n    clipped_poly = poly.intersection(pa_bound.geometry.unary_union)\n    if clipped_poly.is_empty:\n        continue\n    clipped_voronoi_polygons.append(clipped_poly)\n\nvoronoi_sr = gpd.GeoDataFrame(geometry=clipped_voronoi_polygons, crs=pa_bound.crs)\n\n\n\n\nCode\nvoronoi_sr_geom = gpd.sjoin(voronoi_sr,sr_geom, op='intersects')\nvoronoi_sr_geom.drop(columns=['index_right', 'Latitude', 'Longitude'], inplace=True)\n\n\n\n\nCode\nfishpoint_vor_sr = gpd.sjoin(fishpoint, voronoi_sr_geom, how='left', op='intersects')\nfishpoint_vor_sr.drop(columns=['geometry'], inplace=True)\nfishnet_sr = pd.merge(pa_fishnet, fishpoint_vor_sr, on='id', how='left')\n\n\n\n\nCode\nspring_discharge_bynet = pd.merge(spring_discharge_bynet, fishnet_sr[['id', 'Spring']], left_on='id', right_on='id', how='left')\nsummer_discharge_bynet = pd.merge(summer_discharge_bynet, fishnet_sr[['id', 'Summer']], left_on='id', right_on='id', how='left')\nfall_discharge_bynet = pd.merge(fall_discharge_bynet, fishnet_sr[['id', 'Fall']], left_on='id', right_on='id', how='left')\nwinter_discharge_bynet = pd.merge(winter_discharge_bynet, fishnet_sr[['id', 'Winter']], left_on='id', right_on='id', how='left')\n\n\n\n\nCode\nspring_discharge_bynet.rename(columns={'Spring': 'SR'}, inplace=True)\nsummer_discharge_bynet.rename(columns={'Summer': 'SR'}, inplace=True)\nfall_discharge_bynet.rename(columns={'Fall': 'SR'}, inplace=True)\nwinter_discharge_bynet.rename(columns={'Winter': 'SR'}, inplace=True)\n\n\n\n\nLoad Completed Processed Data Below (Optional)\n\n\nCode\n## AQ DATA WITH GEOM\npm25_geom = gpd.read_file('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/pm25_geom.geojson')\no3_geom = gpd.read_file('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/o3_geom.geojson')\nsr_geom = gpd.read_file('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/sr_geom.geojson')\nws_geom = gpd.read_file('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/ws_geom.geojson')\n\n\n\n\nCode\n## FINAL TABLE READY FOR MODELING\nspring_discharge_bynet = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/spring_discharge_bynet.csv')\nsummer_discharge_bynet = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/summer_discharge_bynet.csv')\nfall_discharge_bynet = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/fall_discharge_bynet.csv')\nwinter_discharge_bynet = pd.read_csv('https://raw.githubusercontent.com/emilyzhou112/MUSA6500-Pennsylvania-CRD/main/data/derived/winter_discharge_bynet.csv')"
  },
  {
    "objectID": "projects/PA-CRD-2024.html#dimensionality-reduction",
    "href": "projects/PA-CRD-2024.html#dimensionality-reduction",
    "title": "Spatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures - An Approach Using Earth-Engine-Based Remote Sensing Data and Air Quality Variables in the State of Pennsylvania",
    "section": "6. Dimensionality Reduction",
    "text": "6. Dimensionality Reduction\nThe first step in establishing machine learning models is the selection of the input predictors. The simplest model with the least number of independent variables should be found in order to avoid overfitting. According to some visualizations that we made above, vegetation indices could correlate with each other, same as air quality indicies. Here, we choose the Bayesian information criterion (BIC) was considered to conduct backward elimination, by which the lowest BIC values were used to choose the predictors We use the spring data as an example to select our predictors.\n\n\nCode\nfeature_columns  = ['savi_mean', 'ndvi_mean', 'lst_mean', 'evi_mean', 'band1_mean', 'band2_mean', 'band3_mean', 'band4_mean', 'band5_mean', 'band6_mean', 'band7_mean', 'PM25', 'O3', 'WS', 'SR']\ntarget_column = 'CRD_Rate'\n\nX = spring_discharge_bynet[feature_columns]\ny = spring_discharge_bynet[target_column]\n\n\n\n            \n            \n\n\nThe backward elimination loop below iteratively removes one predictor at a time and refits the model to evaluate the impact on BIC. This backward elimination approach efficiently identifies a subset of predictors that optimizes the balance between model fit and complexity. Eventually, we selected the savi_mean, ndvi_mean, lst_mean, evi_mean, band2_mean, band3_mean, band5_mean, band6_mean, PM25, O3, and WS as our predictor variables.\n\n\nCode\ndef calculate_bic(y_true, y_pred, n, p):\n    \"\"\"Calculate Bayesian Information Criterion (BIC).\"\"\"\n    mse = mean_squared_error(y_true, y_pred)\n    return n * np.log(mse) + p * np.log(n)\n\n# Step 1: Fit the full model\nfull_model = LinearRegression().fit(X, y)\ny_pred_full = full_model.predict(X)\nn_samples = len(y)\np_predictors = X.shape[1]\nbic_full = calculate_bic(y, y_pred_full, n_samples, p_predictors)\n\n# Step 2: Perform backward elimination\nselected_predictors = list(X.columns)\nselected_bic = bic_full\n\nwhile True:\n    best_bic = selected_bic\n    best_predictors = selected_predictors\n\n    for predictor in selected_predictors:\n        predictors_subset = selected_predictors.copy()\n        predictors_subset.remove(predictor)\n        X_subset = X[predictors_subset]\n        model = LinearRegression().fit(X_subset, y)\n        y_pred_subset = model.predict(X_subset)\n        bic = calculate_bic(y, y_pred_subset, n_samples, len(predictors_subset))\n        if bic &lt; best_bic:\n            best_bic = bic\n            best_predictors = predictors_subset\n\n    if best_bic &lt; selected_bic:\n        selected_bic = best_bic\n        selected_predictors = best_predictors\n    else:\n        break\n\n# Print the selected predictors and their BIC\nprint(\"Selected predictors:\", selected_predictors)\nprint(\"BIC:\", selected_bic)\n\n\n\n            \n            \n\n\nSelected predictors: ['savi_mean', 'ndvi_mean', 'lst_mean', 'evi_mean', 'band2_mean', 'band3_mean', 'band5_mean', 'band6_mean', 'PM25', 'O3', 'WS']\nBIC: -15749.311608071926\n\n\nUpon checking the correlation matrix for all of our variables, we found that thereâ€™s strong correlation between bands and between vegetation indicies. Including all of them in our model will lead to overfitting.\n\n\nCode\nspring_discharge_bynet_corr = spring_discharge_bynet[['CRD_Rate','savi_mean', 'ndvi_mean', 'lst_mean', 'evi_mean', 'band1_mean', 'band2_mean', 'band3_mean', 'band4_mean', 'band5_mean', 'band6_mean', 'band7_mean', 'PM25', 'O3', 'WS', 'SR' ]]\n\nplt.figure(figsize=(8,8))\nax = sns.heatmap(spring_discharge_bynet_corr.corr(), annot=True, cmap=\"viridis\")\nplt.show()"
  },
  {
    "objectID": "projects/PA-CRD-2024.html#machine-learning-models",
    "href": "projects/PA-CRD-2024.html#machine-learning-models",
    "title": "Spatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures - An Approach Using Earth-Engine-Based Remote Sensing Data and Air Quality Variables in the State of Pennsylvania",
    "section": "7. Machine Learning Models",
    "text": "7. Machine Learning Models\nWe computed four models, considering different machine learning techniques in order to compare linear and non-linear regression models (support vector regressor, random forest regressor, and multiple layer perceptron). In each model, 80% of the dataset was used as training data, and 20% of the dataset was used as test data.\n\n\nCode\n# selected predictor variables\nfeature_columns  = ['ndvi_mean', 'lst_mean', 'evi_mean', 'band1_mean', 'band3_mean', 'band4_mean', 'band7_mean', 'PM25', 'WS', 'SR']\ntarget_column = 'CRD_Rate'\n\n\n\n            \n            \n\n\nAmong the four models, linear regression is probably the simplest and most common analytic technique used in building a predictive model. It computes a linear relationship between the independent (predictors) and the dependent variables. However, linear regression does not analyze the correlation between predictorsâ€”a major limiting factor when considering remote sensing variables, which are highly correlated. In contrast, multiple layer perceptron with a back-propagation learning process is classified as an artificial neural network (ANN) model, and it can be used in the classification of remote sensing data. It uses a series of neuronal activities where the ideal is to have interconnection weights in a multi-layer perceptron. In this study, our model architecture consists of two dense layers with ReLU activation functions followed by dropout layers to regularize the network and prevent overfitting. The final output layer produces a single numerical prediction.\nSupport vectore regressor is a non-linear transformation of an machine learning techniques which works as a support vector machine classifier. They work in a higher dimensional space. Random forest regressor is based on ensemble learning, which uses the training dataset to generate multiple decision trees, making it less sensitive to the overfitting problem. The decision trees are simply combined according to their weights.\nWe repeate the modeling process for each season four times. For each model at each season, we calculated the mean squared error and the R-squared value.\n\n7.1 Spring Data Models\n\n\nCode\n### Spring Data Machine Learning\nX1 = spring_discharge_bynet[feature_columns]\ny1 = spring_discharge_bynet[target_column]\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=spring_discharge_bynet['Geography'])\n\nscaler = StandardScaler()\nX1_train_scaled = scaler.fit_transform(X1_train)\nX1_test_scaled = scaler.transform(X1_test)\n\n# Linear Regression\nlinear_model = LinearRegression()\nlinear_model.fit(X1_train_scaled, y1_train)\npredictions1_lr = linear_model.predict(X1_test_scaled)\nmse1_lr = mean_squared_error(y1_test, predictions1_lr)\nrmse1_lr = np.sqrt(mse1_lr)\nr21_lr = r2_score(y1_test, predictions1_lr)\nprint(\"RMSE for LR Spring:\", rmse1_lr)\nprint(\"R-squared for LR Spring:\", r21_lr)\n\n# SVM\nsvm_model = SVR()\nsvm_model.fit(X1_train_scaled, y1_train)\npredictions1_svm = svm_model.predict(X1_test_scaled)\nmse1_svm = mean_squared_error(y1_test, predictions1_svm)\nrmse1_svm = np.sqrt(mse1_svm)\nr21_svm = r2_score(y1_test, predictions1_svm)\nprint(\"RMSE for SVM Spring:\", rmse1_svm)\nprint(\"R-squared for SVM Spring:\", r21_svm)\n\n\n# Random Forest\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X1_train_scaled, y1_train)\npredictions1_rf = rf_model.predict(X1_test_scaled)\nmse1_rf = mean_squared_error(y1_test, predictions1_rf)\nrmse1_rf = np.sqrt(mse1_rf)\nr21_rf = r2_score(y1_test, predictions1_rf)\nprint(\"RMSE for RF Spring:\", rmse1_rf)\nprint(\"R-squared for RF Spring:\", r21_rf)\n\n# Multiple Layer Perceptron\nmodel1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation='relu', input_shape=(X1_train_scaled.shape[1],)),\n    tf.keras.layers.Dropout(0.2),  # Dropout layer with dropout rate of 0.2\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.2),  # Dropout layer with dropout rate of 0.2\n    tf.keras.layers.Dense(1)  # Output layer\n])\n\nmodel1.compile(optimizer='adam', loss='mean_squared_error')\nhistory1 = model1.fit(X1_train_scaled, y1_train, epochs=50, batch_size=32, validation_split=0.2)\npredictions1_mlp = model1.predict(X1_test_scaled)\nmse1_mlp = mean_squared_error(y1_test, predictions1_mlp)\nrmse1_mlp = np.sqrt(mse1_mlp)\nr21_mlp = r2_score(y1_test, predictions1_mlp)\nprint(\"R-squared for MLP Spring:\", r21_mlp)\nprint(\"RMSE for MLP Spring:\", rmse1_mlp)\n\n\n\n            \n            \n\n\nRMSE for LR Spring: 0.390315004974376\nR-squared for LR Spring: 0.1211007749107712\nRMSE for SVM Spring: 0.3005158906880084\nR-squared for SVM Spring: 0.47899310576811926\nRMSE for RF Spring: 0.26366126081399055\nR-squared for RF Spring: 0.5989474925016849\nEpoch 1/50\n167/167 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.1957\nEpoch 2/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.2147 - val_loss: 0.1451\nEpoch 3/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1846 - val_loss: 0.1261\nEpoch 4/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1643 - val_loss: 0.1222\nEpoch 5/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1546 - val_loss: 0.1167\nEpoch 6/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1574 - val_loss: 0.1144\nEpoch 7/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1442 - val_loss: 0.1186\nEpoch 8/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1345 - val_loss: 0.1130\nEpoch 9/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1375 - val_loss: 0.1097\nEpoch 10/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1336 - val_loss: 0.1067\nEpoch 11/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.1065\nEpoch 12/50\n167/167 [==============================] - 1s 5ms/step - loss: 0.1267 - val_loss: 0.1072\nEpoch 13/50\n167/167 [==============================] - 1s 7ms/step - loss: 0.1261 - val_loss: 0.1031\nEpoch 14/50\n167/167 [==============================] - 1s 7ms/step - loss: 0.1234 - val_loss: 0.1030\nEpoch 15/50\n167/167 [==============================] - 1s 5ms/step - loss: 0.1184 - val_loss: 0.1055\nEpoch 16/50\n167/167 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.0974\nEpoch 17/50\n167/167 [==============================] - 1s 7ms/step - loss: 0.1179 - val_loss: 0.1033\nEpoch 18/50\n167/167 [==============================] - 1s 5ms/step - loss: 0.1135 - val_loss: 0.0983\nEpoch 19/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 0.0972\nEpoch 20/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 0.0981\nEpoch 21/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1111 - val_loss: 0.0979\nEpoch 22/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1070 - val_loss: 0.0997\nEpoch 23/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1062 - val_loss: 0.0929\nEpoch 24/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.0911\nEpoch 25/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1011 - val_loss: 0.0903\nEpoch 26/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 0.0904\nEpoch 27/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1014 - val_loss: 0.0909\nEpoch 28/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0982 - val_loss: 0.0898\nEpoch 29/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1011 - val_loss: 0.0888\nEpoch 30/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 0.0882\nEpoch 31/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.0866\nEpoch 32/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0967 - val_loss: 0.0869\nEpoch 33/50\n167/167 [==============================] - 1s 3ms/step - loss: 0.0926 - val_loss: 0.0846\nEpoch 34/50\n167/167 [==============================] - 1s 3ms/step - loss: 0.0944 - val_loss: 0.0856\nEpoch 35/50\n167/167 [==============================] - 1s 4ms/step - loss: 0.0940 - val_loss: 0.0862\nEpoch 36/50\n167/167 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0831\nEpoch 37/50\n167/167 [==============================] - 1s 4ms/step - loss: 0.0914 - val_loss: 0.0850\nEpoch 38/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0902 - val_loss: 0.0836\nEpoch 39/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0905 - val_loss: 0.0850\nEpoch 40/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0895 - val_loss: 0.0825\nEpoch 41/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 0.0824\nEpoch 42/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0892 - val_loss: 0.0835\nEpoch 43/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0900 - val_loss: 0.0806\nEpoch 44/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 0.0823\nEpoch 45/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0852 - val_loss: 0.0834\nEpoch 46/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.0878 - val_loss: 0.0796\nEpoch 47/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0840 - val_loss: 0.0790\nEpoch 48/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0862 - val_loss: 0.0804\nEpoch 49/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.0856 - val_loss: 0.0783\nEpoch 50/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0832 - val_loss: 0.0803\n52/52 [==============================] - 0s 1ms/step\nR-squared for MLP Spring: 0.5355138632608012\nRMSE for MLP Spring: 0.2837475267194409\n\n\n\n\n7.2 Summer Data Model\n\n\nCode\n### Summer Data Machine Learning\nX2 = summer_discharge_bynet[feature_columns]\ny2 = summer_discharge_bynet[target_column]\n\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42, stratify=summer_discharge_bynet['Geography'])\n\nscaler = StandardScaler()\nX2_train_scaled = scaler.fit_transform(X2_train)\nX2_test_scaled = scaler.transform(X2_test)\n\n# Linear Regression\nlinear_model.fit(X2_train_scaled, y2_train)\npredictions2_lr = linear_model.predict(X2_test_scaled)\nmse2_lr = mean_squared_error(y2_test, predictions2_lr)\nrmse2_lr = np.sqrt(mse2_lr)\nr22_lr = r2_score(y2_test, predictions2_lr)\n\n\n# SVM\nsvm_model.fit(X2_train_scaled, y2_train)\npredictions2_svm = svm_model.predict(X2_test_scaled)\nmse2_svm = mean_squared_error(y2_test, predictions2_svm)\nrmse2_svm = np.sqrt(mse2_svm)\nr22_svm = r2_score(y2_test, predictions2_svm)\n\n\n\n# Random Forest\nrf_model.fit(X2_train_scaled, y2_train)\npredictions2_rf = rf_model.predict(X2_test_scaled)\nmse2_rf = mean_squared_error(y2_test, predictions2_rf)\nrmse2_rf = np.sqrt(mse2_rf)\nr22_rf = r2_score(y2_test, predictions2_rf)\n\n# Multiple Layer Perceptron\nmodel2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation='relu', input_shape=(X2_train_scaled.shape[1],)),\n    tf.keras.layers.Dropout(0.2),  # Dropout layer with dropout rate of 0.2\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.2),  # Dropout layer with dropout rate of 0.2\n    tf.keras.layers.Dense(1)  # Output layer\n])\n\nmodel2.compile(optimizer='adam', loss='mean_squared_error')\nhistory2 = model2.fit(X2_train_scaled, y2_train, epochs=50, batch_size=32, validation_split=0.2)\npredictions2_mlp = model2.predict(X2_test_scaled)\nmse2_mlp = mean_squared_error(y2_test, predictions2_mlp)\nrmse2_mlp = np.sqrt(mse2_mlp)\nr22_mlp = r2_score(y2_test, predictions2_mlp)\n\n\n\n            \n            \n\n\nEpoch 1/50\n167/167 [==============================] - 2s 6ms/step - loss: 0.3851 - val_loss: 0.1868\nEpoch 2/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.2059 - val_loss: 0.1422\nEpoch 3/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 0.1350\nEpoch 4/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1628 - val_loss: 0.1316\nEpoch 5/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1569 - val_loss: 0.1300\nEpoch 6/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1510 - val_loss: 0.1299\nEpoch 7/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1473 - val_loss: 0.1279\nEpoch 8/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1429 - val_loss: 0.1262\nEpoch 9/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1451 - val_loss: 0.1181\nEpoch 10/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1400 - val_loss: 0.1218\nEpoch 11/50\n167/167 [==============================] - 1s 6ms/step - loss: 0.1368 - val_loss: 0.1175\nEpoch 12/50\n167/167 [==============================] - 1s 6ms/step - loss: 0.1361 - val_loss: 0.1227\nEpoch 13/50\n167/167 [==============================] - 1s 8ms/step - loss: 0.1323 - val_loss: 0.1153\nEpoch 14/50\n167/167 [==============================] - 2s 9ms/step - loss: 0.1280 - val_loss: 0.1150\nEpoch 15/50\n167/167 [==============================] - 1s 9ms/step - loss: 0.1295 - val_loss: 0.1087\nEpoch 16/50\n167/167 [==============================] - 1s 4ms/step - loss: 0.1232 - val_loss: 0.1058\nEpoch 17/50\n167/167 [==============================] - 1s 3ms/step - loss: 0.1265 - val_loss: 0.1090\nEpoch 18/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1258 - val_loss: 0.1091\nEpoch 19/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 0.1075\nEpoch 20/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1208 - val_loss: 0.1088\nEpoch 21/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1197 - val_loss: 0.1061\nEpoch 22/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1177 - val_loss: 0.1025\nEpoch 23/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1143 - val_loss: 0.1023\nEpoch 24/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1170 - val_loss: 0.1018\nEpoch 25/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1141 - val_loss: 0.1023\nEpoch 26/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1136 - val_loss: 0.1000\nEpoch 27/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1124 - val_loss: 0.1006\nEpoch 28/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1120 - val_loss: 0.0978\nEpoch 29/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1113 - val_loss: 0.0995\nEpoch 30/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1068 - val_loss: 0.0981\nEpoch 31/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1084 - val_loss: 0.0962\nEpoch 32/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1045 - val_loss: 0.0945\nEpoch 33/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 0.0976\nEpoch 34/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 0.0950\nEpoch 35/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.0935\nEpoch 36/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 0.0936\nEpoch 37/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.1019 - val_loss: 0.0915\nEpoch 38/50\n167/167 [==============================] - 1s 4ms/step - loss: 0.1053 - val_loss: 0.0947\nEpoch 39/50\n167/167 [==============================] - 1s 4ms/step - loss: 0.1038 - val_loss: 0.0960\nEpoch 40/50\n167/167 [==============================] - 1s 4ms/step - loss: 0.1013 - val_loss: 0.0915\nEpoch 41/50\n167/167 [==============================] - 1s 4ms/step - loss: 0.1006 - val_loss: 0.0920\nEpoch 42/50\n167/167 [==============================] - 1s 3ms/step - loss: 0.1015 - val_loss: 0.0932\nEpoch 43/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0995 - val_loss: 0.0894\nEpoch 44/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0996 - val_loss: 0.0929\nEpoch 45/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0985 - val_loss: 0.0914\nEpoch 46/50\n167/167 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 0.0933\nEpoch 47/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0962 - val_loss: 0.0908\nEpoch 48/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0962 - val_loss: 0.0897\nEpoch 49/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0949 - val_loss: 0.0901\nEpoch 50/50\n167/167 [==============================] - 0s 2ms/step - loss: 0.0952 - val_loss: 0.0899\n52/52 [==============================] - 0s 1ms/step\n\n\n\n\n7.3 Fall Data Model\n\n\nCode\n### Fall Data Machine Learning\nfall_discharge_bynet = fall_discharge_bynet.dropna()\nX3 = fall_discharge_bynet[feature_columns]\ny3 = fall_discharge_bynet[target_column]\n\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=42, stratify=fall_discharge_bynet['Geography'])\n\nscaler = StandardScaler()\nX3_train_scaled = scaler.fit_transform(X3_train)\nX3_test_scaled = scaler.transform(X3_test)\n\n# Linear Regression\nlinear_model.fit(X3_train_scaled, y3_train)\npredictions3_lr = linear_model.predict(X3_test_scaled)\nmse3_lr = mean_squared_error(y3_test, predictions3_lr)\nrmse3_lr = np.sqrt(mse3_lr)\nr23_lr = r2_score(y3_test, predictions3_lr)\n\n\n# SVM\nsvm_model.fit(X3_train_scaled, y3_train)\npredictions3_svm = svm_model.predict(X3_test_scaled)\nmse3_svm = mean_squared_error(y3_test, predictions3_svm)\nrmse3_svm = np.sqrt(mse3_svm)\nr23_svm = r2_score(y3_test, predictions3_svm)\n\n\n\n# Random Forest\nrf_model.fit(X3_train_scaled, y3_train)\npredictions3_rf = rf_model.predict(X3_test_scaled)\nmse3_rf = mean_squared_error(y3_test, predictions3_rf)\nrmse3_rf = np.sqrt(mse3_rf)\nr23_rf = r2_score(y3_test, predictions3_rf)\n\n# Multiple Layer Perceptron\nmodel3 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation='relu', input_shape=(X3_train_scaled.shape[1],)),\n    tf.keras.layers.Dropout(0.2),  # Dropout layer with dropout rate of 0.2\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.2),  # Dropout layer with dropout rate of 0.2\n    tf.keras.layers.Dense(1)  # Output layer\n])\n\nmodel3.compile(optimizer='adam', loss='mean_squared_error')\nhistory3 = model3.fit(X3_train_scaled, y3_train, epochs=50, batch_size=32, validation_split=0.2)\npredictions3_mlp = model3.predict(X3_test_scaled)\nmse3_mlp = mean_squared_error(y3_test, predictions3_mlp)\nrmse3_mlp = np.sqrt(mse3_mlp)\nr23_mlp = r2_score(y3_test, predictions3_mlp)\n\n\n\n            \n            \n\n\nEpoch 1/50\n166/166 [==============================] - 1s 3ms/step - loss: 0.5187 - val_loss: 0.2195\nEpoch 2/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.2418 - val_loss: 0.1581\nEpoch 3/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.2005 - val_loss: 0.1404\nEpoch 4/50\n166/166 [==============================] - 1s 8ms/step - loss: 0.1792 - val_loss: 0.1385\nEpoch 5/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1683 - val_loss: 0.1243\nEpoch 6/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1590 - val_loss: 0.1208\nEpoch 7/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1522 - val_loss: 0.1252\nEpoch 8/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1493 - val_loss: 0.1153\nEpoch 9/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1440 - val_loss: 0.1149\nEpoch 10/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1420 - val_loss: 0.1095\nEpoch 11/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1355 - val_loss: 0.1104\nEpoch 12/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1361 - val_loss: 0.1066\nEpoch 13/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1309 - val_loss: 0.1049\nEpoch 14/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.1293 - val_loss: 0.1115\nEpoch 15/50\n166/166 [==============================] - 3s 20ms/step - loss: 0.1295 - val_loss: 0.1026\nEpoch 16/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.1277 - val_loss: 0.1052\nEpoch 17/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1225 - val_loss: 0.1074\nEpoch 18/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1198 - val_loss: 0.1003\nEpoch 19/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1155 - val_loss: 0.1005\nEpoch 20/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1171 - val_loss: 0.1012\nEpoch 21/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1169 - val_loss: 0.1033\nEpoch 22/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1187 - val_loss: 0.0974\nEpoch 23/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1131 - val_loss: 0.0984\nEpoch 24/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1110 - val_loss: 0.0998\nEpoch 25/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1079 - val_loss: 0.0963\nEpoch 26/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 0.0954\nEpoch 27/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1070 - val_loss: 0.0920\nEpoch 28/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1063 - val_loss: 0.0979\nEpoch 29/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1038 - val_loss: 0.0919\nEpoch 30/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1039 - val_loss: 0.0942\nEpoch 31/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1029 - val_loss: 0.0888\nEpoch 32/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1039 - val_loss: 0.0926\nEpoch 33/50\n166/166 [==============================] - 1s 3ms/step - loss: 0.0995 - val_loss: 0.0889\nEpoch 34/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0979 - val_loss: 0.0895\nEpoch 35/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0977 - val_loss: 0.0875\nEpoch 36/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0986 - val_loss: 0.0869\nEpoch 37/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 0.0889\nEpoch 38/50\n166/166 [==============================] - 1s 3ms/step - loss: 0.0978 - val_loss: 0.0893\nEpoch 39/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.0953 - val_loss: 0.0879\nEpoch 40/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.0966 - val_loss: 0.0884\nEpoch 41/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.0936 - val_loss: 0.0856\nEpoch 42/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.0943 - val_loss: 0.0850\nEpoch 43/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0940 - val_loss: 0.0862\nEpoch 44/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0928 - val_loss: 0.0843\nEpoch 45/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0918 - val_loss: 0.0827\nEpoch 46/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0912 - val_loss: 0.0826\nEpoch 47/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 0.0865\nEpoch 48/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0927 - val_loss: 0.0849\nEpoch 49/50\n166/166 [==============================] - 1s 3ms/step - loss: 0.0897 - val_loss: 0.0831\nEpoch 50/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0887 - val_loss: 0.0810\n52/52 [==============================] - 0s 1ms/step\n\n\n\n\n7.4 Winter Data Model\n\n\nCode\n### Winter Data Machine Learning\nwinter_discharge_bynet = winter_discharge_bynet.dropna()\nX4 = winter_discharge_bynet[feature_columns]\ny4 = winter_discharge_bynet[target_column]\n\n\nX4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=42, stratify=winter_discharge_bynet['Geography'])\nX4_train_scaled = scaler.fit_transform(X4_train)\nX4_test_scaled = scaler.transform(X4_test)\n\n# Linear Regression\nlinear_model.fit(X4_train_scaled, y4_train)\npredictions4_lr = linear_model.predict(X4_test_scaled)\nmse4_lr = mean_squared_error(y4_test, predictions4_lr)\nrmse4_lr = np.sqrt(mse4_lr)\nr24_lr = r2_score(y4_test, predictions4_lr)\n\n\n# SVM\nsvm_model.fit(X4_train_scaled, y4_train)\npredictions4_svm = svm_model.predict(X4_test_scaled)\nmse4_svm = mean_squared_error(y4_test, predictions4_svm)\nrmse4_svm = np.sqrt(mse4_svm)\nr24_svm = r2_score(y4_test, predictions4_svm)\n\n\n\n# Random Forest\nrf_model.fit(X4_train_scaled, y4_train)\npredictions4_rf = rf_model.predict(X4_test_scaled)\nmse4_rf = mean_squared_error(y4_test, predictions4_rf)\nrmse4_rf = np.sqrt(mse4_rf)\nr24_rf = r2_score(y4_test, predictions4_rf)\n\n# Multiple Layer Perceptron\nmodel4 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation='relu', input_shape=(X4_train_scaled.shape[1],)),\n    tf.keras.layers.Dropout(0.2),  # Dropout layer with dropout rate of 0.2\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.2),  # Dropout layer with dropout rate of 0.2\n    tf.keras.layers.Dense(1)  # Output layer\n])\n\nmodel4.compile(optimizer='adam', loss='mean_squared_error')\nhistory4 = model4.fit(X4_train_scaled, y4_train, epochs=50, batch_size=32, validation_split=0.2)\npredictions4_mlp = model4.predict(X4_test_scaled)\nmse4_mlp = mean_squared_error(y4_test, predictions4_mlp)\nrmse4_mlp = np.sqrt(mse4_mlp)\nr24_mlp = r2_score(y4_test, predictions4_mlp)\n\n\n\n            \n            \n\n\nEpoch 1/50\n166/166 [==============================] - 3s 6ms/step - loss: 0.4868 - val_loss: 0.2137\nEpoch 2/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.2333 - val_loss: 0.1615\nEpoch 3/50\n166/166 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.1352\nEpoch 4/50\n166/166 [==============================] - 1s 6ms/step - loss: 0.1723 - val_loss: 0.1328\nEpoch 5/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.1595 - val_loss: 0.1213\nEpoch 6/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.1511 - val_loss: 0.1184\nEpoch 7/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.1503 - val_loss: 0.1153\nEpoch 8/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.1389 - val_loss: 0.1137\nEpoch 9/50\n166/166 [==============================] - 1s 9ms/step - loss: 0.1369 - val_loss: 0.1126\nEpoch 10/50\n166/166 [==============================] - 3s 16ms/step - loss: 0.1363 - val_loss: 0.1107\nEpoch 11/50\n166/166 [==============================] - 2s 12ms/step - loss: 0.1338 - val_loss: 0.1078\nEpoch 12/50\n166/166 [==============================] - 1s 6ms/step - loss: 0.1297 - val_loss: 0.1048\nEpoch 13/50\n166/166 [==============================] - 1s 8ms/step - loss: 0.1269 - val_loss: 0.1055\nEpoch 14/50\n166/166 [==============================] - 1s 6ms/step - loss: 0.1242 - val_loss: 0.1063\nEpoch 15/50\n166/166 [==============================] - 2s 11ms/step - loss: 0.1234 - val_loss: 0.1010\nEpoch 16/50\n166/166 [==============================] - 2s 11ms/step - loss: 0.1180 - val_loss: 0.1020\nEpoch 17/50\n166/166 [==============================] - 2s 14ms/step - loss: 0.1166 - val_loss: 0.1010\nEpoch 18/50\n166/166 [==============================] - 2s 12ms/step - loss: 0.1161 - val_loss: 0.1002\nEpoch 19/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.1148 - val_loss: 0.1001\nEpoch 20/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.1128 - val_loss: 0.1011\nEpoch 21/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.1101 - val_loss: 0.0948\nEpoch 22/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.1091 - val_loss: 0.0968\nEpoch 23/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.1091 - val_loss: 0.0936\nEpoch 24/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.1068 - val_loss: 0.0928\nEpoch 25/50\n166/166 [==============================] - 1s 3ms/step - loss: 0.1044 - val_loss: 0.0922\nEpoch 26/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.0921\nEpoch 27/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 0.0917\nEpoch 28/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1036 - val_loss: 0.0899\nEpoch 29/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.1018 - val_loss: 0.0945\nEpoch 30/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0992 - val_loss: 0.0888\nEpoch 31/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 0.0891\nEpoch 32/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 0.0949\nEpoch 33/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0982 - val_loss: 0.0918\nEpoch 34/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0967 - val_loss: 0.0876\nEpoch 35/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0935 - val_loss: 0.0870\nEpoch 36/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 0.0855\nEpoch 37/50\n166/166 [==============================] - 1s 3ms/step - loss: 0.0946 - val_loss: 0.0892\nEpoch 38/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.0954 - val_loss: 0.0853\nEpoch 39/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0881\nEpoch 40/50\n166/166 [==============================] - 1s 5ms/step - loss: 0.0919 - val_loss: 0.0862\nEpoch 41/50\n166/166 [==============================] - 1s 4ms/step - loss: 0.0921 - val_loss: 0.0861\nEpoch 42/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0929 - val_loss: 0.0848\nEpoch 43/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0902 - val_loss: 0.0846\nEpoch 44/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0900 - val_loss: 0.0837\nEpoch 45/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0903 - val_loss: 0.0828\nEpoch 46/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0874 - val_loss: 0.0821\nEpoch 47/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0882 - val_loss: 0.0821\nEpoch 48/50\n166/166 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 0.0819\nEpoch 49/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0867 - val_loss: 0.0806\nEpoch 50/50\n166/166 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.0803\n52/52 [==============================] - 0s 2ms/step\n\n\n\n\n7.5 Model Comparison and Evaluations\nHere, we first present the structure of the multiple layer perceptron. Then the chart and two tables show the comparision between R-Square and RMSE for all models established for each season.\n\n\nCode\nmodel1.summary()\n\n\n\n            \n            \n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 128)               1408      \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense_1 (Dense)             (None, 64)                8256      \n                                                                 \n dropout_1 (Dropout)         (None, 64)                0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 9729 (38.00 KB)\nTrainable params: 9729 (38.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nCode\nresults_rmse = {\n    \"Model\": [\"LR\", \"SVM\", \"RF\", \"MLP\"],\n    \"Spring_RMSE\": [rmse1_lr, rmse1_svm, rmse1_rf, rmse1_mlp],\n    \"Summer_RMSE\": [rmse2_lr, rmse2_svm, rmse2_rf, rmse2_mlp],\n    \"Fall_RMSE\": [rmse3_lr, rmse3_svm, rmse3_rf, rmse3_mlp],\n    \"Winter_RMSE\": [rmse4_lr, rmse4_svm, rmse4_rf, rmse4_mlp],\n}\nresults_rmse_df = pd.DataFrame(results_rmse)\nresults_rmse_df.set_index('Model', inplace=True)\nresults_rmse_df.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\nSpring_RMSE\nSummer_RMSE\nFall_RMSE\nWinter_RMSE\n\n\nModel\n\n\n\n\n\n\n\n\nLR\n0.390315\n0.394325\n0.407834\n0.389794\n\n\nSVM\n0.300516\n0.330741\n0.320876\n0.320580\n\n\nRF\n0.263661\n0.296944\n0.276399\n0.285896\n\n\nMLP\n0.282484\n0.304782\n0.294058\n0.298335\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nCode\nresults_rs = {\n    \"Model\": [\"LR\", \"SVM\", \"RF\", \"MLP\"],\n    \"Spring_R2\": [r21_lr, r21_svm, r21_rf, r21_mlp],\n    \"Summer_R2\": [r22_lr, r22_svm, r22_rf, r22_mlp],\n    \"Fall_R2\": [r23_lr, r23_svm, r23_rf, r23_mlp],\n    \"Winter_R2\": [r24_lr, r24_svm, r24_rf, r24_mlp]\n}\nresults_rs_df = pd.DataFrame(results_rs)\nresults_rs_df.set_index('Model', inplace=True)\nresults_rs_df.head()\n\n\n\n            \n            \n\n\n\n  \n    \n\n\n\n\n\n\nSpring_R2\nSummer_R2\nFall_R2\nWinter_R2\n\n\nModel\n\n\n\n\n\n\n\n\nLR\n0.121101\n0.102949\n0.041985\n0.124707\n\n\nSVM\n0.478993\n0.368920\n0.406966\n0.407952\n\n\nRF\n0.598947\n0.491303\n0.559975\n0.529131\n\n\nMLP\n0.539641\n0.464095\n0.501951\n0.487265\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThe non-linear random forest model showed the best adjustment in the test data.The model presents the lowest RMSE (0.263661 for spring, 0.296944 for summer, 0.276399 for fall, and 0.285896 for winter) as well as the highest R2 (0.598947 for spring, 0.491303 for summer, 0.559975 for fall, and 0.529131 for winter) across all seasons. In particular, the RMSE is lowest for spring but highest for summer.\nThis is likley due to: - seasonal allergens and pollutants: spring is typically associated with an increase in allergens like pollen, which might have a stronger impact on respiratory health. The increase in allergens is indeed reflected in the vegetation covers and possibly wind speed from that season. - human behavior, such as spending more time outdoors could lead to different exposure patterns to environmental factors and pollutants, affecting the relationship between remote sensing data and health outcomes.\nThe overall accuracy of random forest models is followed by multiple layer perceptron, support vector regression, and finally the linear regression model. The superior performance of the random forest model compared to other models can be attributed to its non-linearity, robustness, and ability to handle high-dimensional data.\n\n\nCode\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n\n# Plot RMSE values\nresults_rmse_df.plot(kind='bar', ax=axes[0], title='RMSE by Model and Season', rot=0, cmap = 'viridis')\naxes[0].set_ylabel('RMSE')\n\n# Plot R-squared values\nresults_rs_df.plot(kind='bar', ax=axes[1], title='R-squared by Model and Season', rot=0, cmap = 'viridis')\naxes[1].set_ylabel('R-squared')\n\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nWe also compared the distribution of the predicted and actual CRD hospital discharge values produced by the random forest model. This gives us a clearer view of the extent to which our model has over or under predict hospital discharge. For all four seasons, we can see that our model performs pretty well according to the normal distributions and indeed, the distribution curve of the spring model best matches the original curve.\n\n\nCode\nfinal_sp = pd.DataFrame({'Original': y1_test, 'Predicted': predictions1_rf})\nfinal_sm = pd.DataFrame({'Original': y2_test, 'Predicted': predictions2_rf})\nfinal_fa = pd.DataFrame({'Original': y3_test, 'Predicted': predictions3_rf})\nfinal_wi = pd.DataFrame({'Original': y4_test, 'Predicted': predictions4_rf})\n\n\n\n            \n            \n\n\n\n\nCode\ndataframes = [final_sp, final_sm, final_fa, final_wi]\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\nseasons = ['Spring', 'Summer', 'Autumn', 'Winter']\nfrom matplotlib.cm import viridis\n\nfor i, df in enumerate(dataframes):\n    data1 = df[['Original']]\n    mu1, std1 = norm.fit(data1)\n\n    data2 = df[['Predicted']]\n    mu2, std2 = norm.fit(data2)\n\n    row = i // 2\n    col = i % 2\n\n    color1 = viridis(i/len(dataframes)*0.8)\n    color2 = viridis(i/len(dataframes)*0.6)\n\n    axes[row, col].hist(data1, bins=30, density=True, alpha=0.6, color=color1, label='Original CRD')\n    axes[row, col].hist(data2, bins=30, density=True, alpha=0.8, color=color2, label='Predicted CRD')\n\n    xmin, xmax = axes[row, col].get_xlim()\n    x1 = np.linspace(xmin, xmax, 100)\n    p1 = norm.pdf(x1, mu1, std1)\n    axes[row, col].plot(x1, p1, 'k--', linewidth=2)\n\n    x2 = np.linspace(xmin, xmax, 100)\n    p2 = norm.pdf(x2, mu2, std2)\n    axes[row, col].plot(x2, p2, 'r--', linewidth=2)\n\n    axes[row, col].set_xlabel('Value')\n    axes[row, col].set_ylabel('Density')\n    axes[row, col].set_title(seasons[i])\n    axes[row, col].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nWe would like to identify any over or underpredictions of our models. We may see that for each season, there are a few cases where CRD discharge has been significantly over/under predicted. The difference between each season is nuanced according to the visualization, but it seems like our models tend to underpredict CRD rates that were originally high.\n\n\nCode\ndataframes = [final_sp, final_sm, final_fa, final_wi]\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\nseasons = ['Spring', 'Summer', 'Autumn', 'Winter']\nfrom matplotlib.cm import viridis\n\nfor i, df in enumerate(dataframes):\n    data1 = df['Original']\n    mu1, std1 = norm.fit(data1)\n\n    data2 = df['Predicted']\n    mu2, std2 = norm.fit(data2)\n\n    row = i // 2\n    col = i % 2\n\n    color1 = viridis(i/len(dataframes)*0.8)\n    color2 = viridis(i/len(dataframes)*0.6)\n\n    axes[row, col].scatter(data1, data2, color=color1, alpha=0.6, label='Original vs Predicted', edgecolors='w')\n\n    xmin, xmax = min(data1.min(), data2.min()), max(data1.max(), data2.max())\n    x = np.linspace(xmin, xmax, 100)\n    axes[row, col].plot(x, x, 'k--', linewidth=2)\n\n    axes[row, col].set_xlabel('Original')\n    axes[row, col].set_ylabel('Predicted')\n    axes[row, col].set_title(seasons[i])\n    axes[row, col].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nWe selected those underpredicted values from the spring random forest model to take a closer look the counties at which it falls in. We would like to focus on underpredicted values because we want to be informed about the potential risks of CRD based on the local environment. Having a model that significantly underpredicts CRD rate will let us overlook its potential impact. We found that the following counties, especially Columbia, Lebanon, McKean, Wayne, Junita, and Jefferson, seems to contain a significant number of grids that have higher CRD rates but are underpredicted.\n\n\nCode\ntarget_column_map = ['CRD_Rate', 'Geography']\ny1 = spring_discharge_bynet[target_column_map]\n\nX1_train, X1_test, y1_train, y1_test_2 = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=spring_discharge_bynet['Geography'])\noriginal_rate = y1_test_2['CRD_Rate']\ngeography = y1_test_2['Geography']\n\nfinal_sp = pd.DataFrame({'Original': original_rate, 'County': geography, 'Predicted': predictions1_rf})\n\n\n\n\nCode\nselected_rows = final_sp[final_sp['Original'].astype(float) &gt; final_sp['Predicted']]\n\n\n\n\nCode\ncounty_rank = selected_rows.groupby('County').size().rank(ascending=False)\nsorted_df = county_rank.sort_values(ascending=False)\n\n\n\n\nCode\nsorted_df.plot(kind='bar', figsize=(10, 6), cmap = 'viridis', width=0.8)\n\n# Add labels and title\nplt.xlabel('County')\nplt.ylabel('Rank')\nplt.title('Rank of Counties based on Number of Underpredictions')\nplt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n\n# Show plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/PA-CRD-2024.html#conclusions",
    "href": "projects/PA-CRD-2024.html#conclusions",
    "title": "Spatial Estimation of Chronic Respiratory Disease Based on Geospatial Machine Learning Procedures - An Approach Using Earth-Engine-Based Remote Sensing Data and Air Quality Variables in the State of Pennsylvania",
    "section": "8. Conclusions",
    "text": "8. Conclusions\nIn summary, we investigated the effectiveness of four machine learning models, linear regression, support vector regression, random forest regression, and multiple layer perceptron, in predicting hospitalization discharge of patients with chronic respiratory disease in the state of Pennsylvania using a combination of remote sensing data and air quality variables. Drawing on the methodologies and workflows of Alvarez-Mendoza et al., we acquired the average of all Landsat 8 level 2 images from Spring 2022 to Spring 2023 by season via the Google Earth Engine API, from which we computed the vegetation indices (NDVI, EVI, and SAVI) and land surface temperature. We took the mean of air quality field measurements from stations across Pennsylvania and summarized the average wind speed, PM2.5, solar radiation, and ozone level by season. We aggregated all variables into a fishnet grid of Pennsylvania, on which we ran our models by season and computed the RMSE and R-squared.\nUpon examining our input data, we have found that there are significant urban-rural differences in CRD risks, where hospitalization due to CRD is much higher in Philadelphia and surrounding counties as well as in Pittsburgh. In addition to that, a few counties in the northern and central part of the state also stand out as having higher CRD risks. Similarly, vegetation indices are higher in the southeastern and southwestern part of the state but lower in the central part of the state. PM2.5 tends to be higher in winter and summer while lower in spring and fall. Some counties in the northern and western part of the state stand out.\nAmong the four models, we found that the non-linear random forest model showed the best adjustment in the test data. The model presents the lowest RMSE and the highest R-Squared across all seasons. The model performs the best in spring and worst in summer. The overall accuracy of the random forest model is followed by multiple layer perceptron, support vector regression, and finally the linear regression one. In this case, random forest is the more robust method because it is a method that combines multiple decision trees. Each tree is trained on a random subset of features and observations. This randomness helps reduce the variance of the model, making it less susceptible to overfitting, especially in high-dimensional spaces. We delved further into the accuracy of this random forest model by looking at the degree to which it over It seems that the random forest models are over-predicting CRD hospitalization discharge thatâ€™s in the middle range, but under-predicting discharges of grids that had extremely high or low actual discharges. This would be problematic as for healthcare professionals and policy makers, an ideal model would be the one that do not underestimate the risks.\nAs such, we dug further into the counties where underprediction most often occurs. Weâ€™ve found the following counties, especially Columbia, Lebanon, McKean, Wayne, Junita, and Jefferson, seem to experience the most significant number of underpredictions. Most of these counties are located at central or northern Pennsylvania and are less populated. We believe that more attention should be given to these counties in terms of the occurrence of CRD in relation to the local environment.\nReflecting on the overall study design, there are a couple of limitations that we have to acknowledge and work towards addressing them in future attempts. Firstly, we chose to compute Voronoi polygons to characterize the influence area of air quality monitoring stations. While Voronoi polygons is good at making spatial interpolations such that each station will be associated with a polygon representing the area where it has the closet proximity, it does not take into account local geographies. For example, a station located on one side of a hill might be collecting air quality measures that is completely different from what is a happening on the other side of the hill. Nevertheless, the Voronoi polygon algorithm will group the areas on both side of the hill into one influence area. This will definitely introduce errors into our model.\nSecondly, missing data is a concern in this project. Thereâ€™s one county in Pennsylvania with missing hospital discharge data. We originally expected each county in Pennsylvania to have their own air quality data collecting station. However, upon examining the data, we found that this is not the case. On top of that, different stations could be collecting completely different air quality measures. Some stations are only collecting for half of the month while other might be missing data for an entire month. We also found that Philadelphia Countyâ€™s air quality data is maintained separately than other counties and are stored in a different format. This leads us to make a lot of compromises assumptions based on the available data and previous yearsâ€™ record in order to determine an approximate value for the month we need.\nThirdly, the spatial unit of our analysis could potentially introduce uncertainties into our model. There has been literature that questions the accuracy of using fishnet versus using hexagons. Fishnet grids can draw our eyes to the straight, unbroken, parallel lines which may inhibit the underlying patterns in the data, whereas hexagons tend to break up the lines and allow any curvature of the patterns in the data to be seen more clearly and easily. While using a standardize spatial unit makes the modeling process easier, we have to do several rounds of area-weighted reaggregation to aggregate data at the county or measuring station influence area level into the fishnet. The hospital data is at the county level, but we believe that there will certainly be intra-county variations in terms of the CRD discharges that this county level data had missed.\nFourthly, in terms of the scope of the analysis, we hope we could improve upon the original paper by extending the temporal scale of analysis to include multiple years of data, rather than merely accounting for seasonal variations (especially if the hospital data are not at seasonal level). It will be great to look at how previous years data could inform us about future trends of hospital discharge, though this would require a lot more effort in collecting and cleaning the air quality data given all its limitations at this point. We would also like to include more machine learning models for comparison to see how linear/non-linear model perform in making predictions.\nRegardless, the takeaway from this project will contribute to our understanding of spatial and temporal variations in CRD risks in relation to local environment in the state of Pennsylvania. It also improves our understanding of the significant environmental and atmospheric factors leading to higher CRD risks in Pennsylvania as well as the power of different machine learning models in making future predictions. The random forest model outperformed other models in this project, especially when being run on the spring data, and we have identified several counties where under-predictions is an issues. We hope that our attempt to improve on Alvarez-Mendoza et al.â€™s study could provide an approach of spatial distribution of air pollutants and its possible relationship with epidemiological data, serving a feasible alternative to the governmental health institutions."
  }
]